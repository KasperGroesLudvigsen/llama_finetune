
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.7632411631619961, "macro_f1": 0.713263857944391}, {"mcc": 0.7546156425407411, "macro_f1": 0.6998441891698756}, {"mcc": 0.7753821652443857, "macro_f1": 0.6897325170899221}, {"mcc": 0.8100709411625724, "macro_f1": 0.7814970546519401}, {"mcc": 0.7306769410774258, "macro_f1": 0.6160563286221415}, {"mcc": 0.7567565310991968, "macro_f1": 0.7064619944288874}, {"mcc": 0.7764128852546965, "macro_f1": 0.7300789777321789}, {"mcc": 0.7759392333123692, "macro_f1": 0.7514587564194685}, {"mcc": 0.7675303731147312, "macro_f1": 0.6657242799149489}, {"mcc": 0.7868790013144131, "macro_f1": 0.7842388068461238}]}, "total": {"test_mcc": 76.97504877282529, "test_mcc_se": 1.3075610283923544, "test_macro_f1": 71.38356762819878, "test_macro_f1_se": 3.1953900405412945}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.399072095776686, "macro_f1": 0.4956553080344501}, {"mcc": 0.4152435268624868, "macro_f1": 0.5279818535078712}, {"mcc": 0.4725509003311654, "macro_f1": 0.6134480203206284}, {"mcc": 0.4450589416808148, "macro_f1": 0.586510862433019}, {"mcc": 0.4817215855063483, "macro_f1": 0.6211363548687684}, {"mcc": 0.44321075474024424, "macro_f1": 0.5515259492902045}, {"mcc": 0.444891680376046, "macro_f1": 0.5740848637429616}, {"mcc": 0.4292713028314237, "macro_f1": 0.5250211388231241}, {"mcc": 0.3221088910654179, "macro_f1": 0.3917474516983129}, {"mcc": 0.42367754777904576, "macro_f1": 0.5826489283257223}]}, "total": {"test_mcc": 42.768072269496784, "test_mcc_se": 2.7649398203380207, "test_macro_f1": 54.697607310450614, "test_macro_f1_se": 4.187553369717499}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.42687354483398093, "macro_f1": 0.615133954516642}, {"mcc": 0.45786901368589694, "macro_f1": 0.6151626832347602}, {"mcc": 0.42933201238141316, "macro_f1": 0.5741198849104404}, {"mcc": 0.4987482981175477, "macro_f1": 0.6607873685687576}, {"mcc": 0.4122372239754492, "macro_f1": 0.5835747407709193}, {"mcc": 0.38960596999656755, "macro_f1": 0.5471449074961574}, {"mcc": 0.5018619386100225, "macro_f1": 0.6567634298473779}, {"mcc": 0.44766576288656307, "macro_f1": 0.5904603754313674}, {"mcc": 0.36666100860478645, "macro_f1": 0.5099896068699068}, {"mcc": 0.48182858418983704, "macro_f1": 0.6563055368801076}]}, "total": {"test_mcc": 44.12683357282064, "test_mcc_se": 2.799814907186832, "test_macro_f1": 60.094424885264374, "test_macro_f1_se": 3.0965269195651484}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.5263632377901644, "macro_f1": 0.6535179156621208}, {"mcc": 0.5336010077951835, "macro_f1": 0.6512171830560601}, {"mcc": 0.4641555862200793, "macro_f1": 0.5422863101976921}, {"mcc": 0.4510916017376896, "macro_f1": 0.6076436945931389}, {"mcc": 0.5381482395157077, "macro_f1": 0.6562727058313906}, {"mcc": 0.4924504450293825, "macro_f1": 0.5828058407629721}, {"mcc": 0.5248759145946802, "macro_f1": 0.6528519810447487}, {"mcc": 0.5620983708430101, "macro_f1": 0.654199399810668}, {"mcc": 0.46877605450741294, "macro_f1": 0.5798069544487294}, {"mcc": 0.4906949153296875, "macro_f1": 0.6580787009171929}]}, "total": {"test_mcc": 50.52255373362997, "test_mcc_se": 2.2891536897336633, "test_macro_f1": 62.386806863247145, "test_macro_f1_se": 2.6258696538849184}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.12842761521159515, "macro_f1": 0.385894483867601}, {"mcc": 0.09785289685345999, "macro_f1": 0.3545215763378195}, {"mcc": 0.11953709613642809, "macro_f1": 0.3879978404992488}, {"mcc": 0.16309648336494623, "macro_f1": 0.4073904248447237}, {"mcc": 0.13400453346634456, "macro_f1": 0.33943857971169633}, {"mcc": 0.1426755105877787, "macro_f1": 0.3628458993313086}, {"mcc": 0.15858394054912964, "macro_f1": 0.38149600628781694}, {"mcc": 0.12324241903962806, "macro_f1": 0.32891750838673534}, {"mcc": 0.11210498237503322, "macro_f1": 0.3823034790443671}, {"mcc": 0.10776931340128468, "macro_f1": 0.3925777627854465}]}, "total": {"test_mcc": 12.872947909856283, "test_mcc_se": 1.3194199656381653, "test_macro_f1": 37.23383561096764, "test_macro_f1_se": 1.5496186720345828}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.678825382332653, "macro_f1": 0.7147301822357218}, {"mcc": 0.645994697826103, "macro_f1": 0.6979712964319087}, {"mcc": 0.6632420143908082, "macro_f1": 0.7043472442317321}, {"mcc": 0.6463534943546901, "macro_f1": 0.7167156150605861}, {"mcc": 0.6828324718047131, "macro_f1": 0.7171584314305689}, {"mcc": 0.6465468092401774, "macro_f1": 0.6654606486637632}, {"mcc": 0.681253608452132, "macro_f1": 0.7251872775019484}, {"mcc": 0.6579354351306118, "macro_f1": 0.7026860531959161}, {"mcc": 0.6837581445130559, "macro_f1": 0.6991617544948219}, {"mcc": 0.6130261061784783, "macro_f1": 0.7033024820582853}]}, "total": {"test_mcc": 65.99768164223423, "test_mcc_se": 1.412856626542079, "test_macro_f1": 70.46720985305252, "test_macro_f1_se": 1.0208990538810823}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.38148788927335636, "micro_f1": 0.30680218921032054}, {"micro_f1_no_misc": 0.44143723751749886, "micro_f1": 0.30749842470069316}, {"micro_f1_no_misc": 0.4495816820783795, "micro_f1": 0.3869596031183558}, {"micro_f1_no_misc": 0.42835265806169326, "micro_f1": 0.32641221374045803}, {"micro_f1_no_misc": 0.4262425447316104, "micro_f1": 0.33172932330827065}, {"micro_f1_no_misc": 0.4347826086956522, "micro_f1": 0.3827591945415211}, {"micro_f1_no_misc": 0.5202284626368396, "micro_f1": 0.3861185983827493}, {"micro_f1_no_misc": 0.37607573149741824, "micro_f1": 0.27114210985178727}, {"micro_f1_no_misc": 0.4757536041939711, "micro_f1": 0.3901382642012327}, {"micro_f1_no_misc": 0.4398375721307972, "micro_f1": 0.34757468679730164}]}, "total": {"test_micro_f1_no_misc": 43.737799908172164, "test_micro_f1_no_misc_se": 2.5806424133895067, "test_micro_f1": 34.371346078526905, "test_micro_f1_se": 2.5925637979218137}}, "num_model_parameters": 3212749824, "max_sequence_length": 131200, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.49785038693035255, "micro_f1": 0.38921599008366903}, {"micro_f1_no_misc": 0.4097273397199705, "micro_f1": 0.3515554213228632}, {"micro_f1_no_misc": 0.38207200587803086, "micro_f1": 0.3264340626848019}, {"micro_f1_no_misc": 0.36854866643574646, "micro_f1": 0.3087248322147651}, {"micro_f1_no_misc": 0.48003227107704716, "micro_f1": 0.38395245170876674}, {"micro_f1_no_misc": 0.4091097308488613, "micro_f1": 0.32336790726052467}, {"micro_f1_no_misc": 0.3548732595501607, "micro_f1": 0.25571875752468093}, {"micro_f1_no_misc": 0.38550826591628556, "micro_f1": 0.276997578692494}, {"micro_f1_no_misc": 0.3483825097760398, "micro_f1": 0.2873722022658193}, {"micro_f1_no_misc": 0.47626774847870174, "micro_f1": 0.346797461050202}]}, "total": {"test_micro_f1_no_misc": 41.12372184611197, "test_micro_f1_no_misc_se": 3.390597785070832, "test_micro_f1": 32.50136664808586, "test_micro_f1_se": 2.738470519394666}}, "num_model_parameters": 3212749824, "max_sequence_length": 131200, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.48303186907838075, "micro_f1": 0.3824583075972823}, {"micro_f1_no_misc": 0.49772074962012497, "micro_f1": 0.44428871758934835}, {"micro_f1_no_misc": 0.46096291476903056, "micro_f1": 0.33023412120533213}, {"micro_f1_no_misc": 0.5143249270886945, "micro_f1": 0.403911244828883}, {"micro_f1_no_misc": 0.47797645006541656, "micro_f1": 0.424330900243309}, {"micro_f1_no_misc": 0.4776559174987723, "micro_f1": 0.388515577275504}, {"micro_f1_no_misc": 0.513619954053167, "micro_f1": 0.45094894413258485}, {"micro_f1_no_misc": 0.5115264797507788, "micro_f1": 0.3339595074097266}, {"micro_f1_no_misc": 0.47109067017082784, "micro_f1": 0.3700061462814997}, {"micro_f1_no_misc": 0.5584574934268186, "micro_f1": 0.4019502913545011}]}, "total": {"test_micro_f1_no_misc": 49.663674255220116, "test_micro_f1_no_misc_se": 1.7850210381146883, "test_micro_f1": 39.306037579179716, "test_micro_f1_se": 2.551737546519978}}, "num_model_parameters": 3212749824, "max_sequence_length": 131200, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.5036496350364963, "micro_f1": 0.440620863915293}, {"micro_f1_no_misc": 0.5532886723507917, "micro_f1": 0.46334241383394525}, {"micro_f1_no_misc": 0.48470588235294115, "micro_f1": 0.4326934984520123}, {"micro_f1_no_misc": 0.5184388444990782, "micro_f1": 0.3372004564473184}, {"micro_f1_no_misc": 0.4966078697421981, "micro_f1": 0.33074792243767315}, {"micro_f1_no_misc": 0.5439474507350641, "micro_f1": 0.4640365648304065}, {"micro_f1_no_misc": 0.5055605650736399, "micro_f1": 0.46756550496056987}, {"micro_f1_no_misc": 0.5205351586807716, "micro_f1": 0.4482003891050583}, {"micro_f1_no_misc": 0.5114803625377644, "micro_f1": 0.44171031794372034}, {"micro_f1_no_misc": 0.5601640119854914, "micro_f1": 0.42212976498369503}]}, "total": {"test_micro_f1_no_misc": 51.98378452994237, "test_micro_f1_no_misc_se": 1.5513753294819665, "test_micro_f1": 42.482476969096915, "test_micro_f1_se": 3.1008672620851705}}, "num_model_parameters": 3212749824, "max_sequence_length": 131200, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.3122942607700014, "micro_f1": 0.21774046358824606}, {"micro_f1_no_misc": 0.30607561299673874, "micro_f1": 0.2695363572362466}, {"micro_f1_no_misc": 0.28886581884246043, "micro_f1": 0.22982975573649148}, {"micro_f1_no_misc": 0.24806895779693272, "micro_f1": 0.22442918136253945}, {"micro_f1_no_misc": 0.25406787540678755, "micro_f1": 0.21166191155492153}, {"micro_f1_no_misc": 0.31602768903088385, "micro_f1": 0.24470633323752033}, {"micro_f1_no_misc": 0.261994473750314, "micro_f1": 0.22023476802683067}, {"micro_f1_no_misc": 0.2502768549280177, "micro_f1": 0.21597633136094677}, {"micro_f1_no_misc": 0.25011579434923575, "micro_f1": 0.2113567562608935}, {"micro_f1_no_misc": 0.2688122250520954, "micro_f1": 0.20679320679320679}]}, "total": {"test_micro_f1_no_misc": 27.565995629234674, "test_micro_f1_no_misc_se": 1.7073210285581408, "test_micro_f1": 22.52265065157843, "test_micro_f1_se": 1.177329676367647}}, "num_model_parameters": 3212749824, "max_sequence_length": 131200, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.5021744106202792, "micro_f1": 0.37561359917580756}, {"micro_f1_no_misc": 0.5390037221826245, "micro_f1": 0.43123312923826457}, {"micro_f1_no_misc": 0.5987734300768766, "micro_f1": 0.40811808118081183}, {"micro_f1_no_misc": 0.5700508051321795, "micro_f1": 0.4478378565974873}, {"micro_f1_no_misc": 0.6130723523243621, "micro_f1": 0.46321450734330827}, {"micro_f1_no_misc": 0.5213247930010936, "micro_f1": 0.4460617603047205}, {"micro_f1_no_misc": 0.6039943177070276, "micro_f1": 0.5159231048240841}, {"micro_f1_no_misc": 0.6664885088188136, "micro_f1": 0.6173793992107595}, {"micro_f1_no_misc": 0.6774574270780643, "micro_f1": 0.5926307273663308}, {"micro_f1_no_misc": 0.5318334505116787, "micro_f1": 0.44632504831078823}]}, "total": {"test_micro_f1_no_misc": 58.24173217452999, "test_micro_f1_no_misc_se": 3.7269232239835643, "test_micro_f1": 47.44337213552362, "test_micro_f1_se": 4.8268519707174145}}, "num_model_parameters": 3212749824, "max_sequence_length": 131200, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.5501664289110795, "micro_f1": 0.48989706443004194}, {"micro_f1_no_misc": 0.5845232611699678, "micro_f1": 0.4673444769946579}, {"micro_f1_no_misc": 0.5644394110985278, "micro_f1": 0.4492630865661528}, {"micro_f1_no_misc": 0.5519287833827893, "micro_f1": 0.44889357218124337}, {"micro_f1_no_misc": 0.5817998487522058, "micro_f1": 0.41869557490669984}, {"micro_f1_no_misc": 0.5404818754069893, "micro_f1": 0.472654835204636}, {"micro_f1_no_misc": 0.5178219986879511, "micro_f1": 0.4207044815611047}, {"micro_f1_no_misc": 0.6005323009920155, "micro_f1": 0.5270606234461657}, {"micro_f1_no_misc": 0.5713032031592805, "micro_f1": 0.5037974683544304}, {"micro_f1_no_misc": 0.4889490302210194, "micro_f1": 0.4200426439232409}]}, "total": {"test_micro_f1_no_misc": 55.51946141781826, "test_micro_f1_no_misc_se": 2.066648301392786, "test_micro_f1": 46.18353827568373, "test_micro_f1_se": 2.316801201481415}}, "num_model_parameters": 3212749824, "max_sequence_length": 131200, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.44666923373122835, "micro_f1": 0.41420118343195267}, {"micro_f1_no_misc": 0.43954372623574145, "micro_f1": 0.4231611893583725}, {"micro_f1_no_misc": 0.4554951943167572, "micro_f1": 0.4268723882995821}, {"micro_f1_no_misc": 0.39088905216752395, "micro_f1": 0.3630843958712811}, {"micro_f1_no_misc": 0.4309210526315789, "micro_f1": 0.37780548628428934}, {"micro_f1_no_misc": 0.4395161290322581, "micro_f1": 0.4003777148253069}, {"micro_f1_no_misc": 0.42512783053323594, "micro_f1": 0.37530201342281877}, {"micro_f1_no_misc": 0.3947655398037077, "micro_f1": 0.39581464872944694}, {"micro_f1_no_misc": 0.5074626865671641, "micro_f1": 0.45633802816901403}, {"micro_f1_no_misc": 0.4353083434099153, "micro_f1": 0.3903037741638539}]}, "total": {"test_micro_f1_no_misc": 43.65698788429111, "test_micro_f1_no_misc_se": 2.0139460451621383, "test_micro_f1": 40.23260822555918, "test_micro_f1_se": 1.7478506680213506}}, "num_model_parameters": 3212749824, "max_sequence_length": 131200, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.6699196326061998, "micro_f1": 0.5391575200237319}, {"micro_f1_no_misc": 0.685576566555432, "micro_f1": 0.6366903468449644}, {"micro_f1_no_misc": 0.7229476667678915, "micro_f1": 0.5877693282636248}, {"micro_f1_no_misc": 0.6558209516644724, "micro_f1": 0.5470764617691154}, {"micro_f1_no_misc": 0.6938241614500609, "micro_f1": 0.6105313092979128}, {"micro_f1_no_misc": 0.683188211654387, "micro_f1": 0.5611392782582761}, {"micro_f1_no_misc": 0.6765871098656874, "micro_f1": 0.5791026238161775}, {"micro_f1_no_misc": 0.670175120225025, "micro_f1": 0.5055224579958498}, {"micro_f1_no_misc": 0.69105222734255, "micro_f1": 0.5561232156273479}, {"micro_f1_no_misc": 0.6949596969991259, "micro_f1": 0.5572687224669604}]}, "total": {"test_micro_f1_no_misc": 68.44051345130832, "test_micro_f1_no_misc_se": 1.1359700272037216, "test_micro_f1": 56.80381264363962, "test_micro_f1_se": 2.3115234574845704}}, "num_model_parameters": 3212749824, "max_sequence_length": 131200, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.1983356821544958, "macro_f1": 0.5886111986219216}, {"mcc": 0.1660396342464956, "macro_f1": 0.47781353686946915}, {"mcc": 0.1570022003444398, "macro_f1": 0.4786738602311827}, {"mcc": 0.14855414929809418, "macro_f1": 0.5509702687198339}, {"mcc": 0.21304074513090104, "macro_f1": 0.5567338282078473}, {"mcc": 0.12809183272736183, "macro_f1": 0.4190994044676694}, {"mcc": 0.12435967308303233, "macro_f1": 0.451274149257646}, {"mcc": 0.12409156080483984, "macro_f1": 0.5469485165442902}, {"mcc": 0.21246669200220739, "macro_f1": 0.558456072567105}, {"mcc": 0.12853088922031627, "macro_f1": 0.553983418917015}]}, "total": {"test_mcc": 16.00513059012184, "test_mcc_se": 2.2423927704785074, "test_macro_f1": 51.825642544039795, "test_macro_f1_se": 3.5023366934927114}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.12125042746401757, "macro_f1": 0.47322067821889313}, {"mcc": 0.10052477241292339, "macro_f1": 0.43733740019736256}, {"mcc": 0.02480578826087154, "macro_f1": 0.3783652743875743}, {"mcc": 0.1676475925141546, "macro_f1": 0.5640943587798624}, {"mcc": 0.06504230158521394, "macro_f1": 0.4566036266129767}, {"mcc": 0.13579783296949066, "macro_f1": 0.5113258400363045}, {"mcc": 0.14294570424682812, "macro_f1": 0.5237700988704463}, {"mcc": 0.19180423003059308, "macro_f1": 0.5956969558251316}, {"mcc": 0.09456945317594954, "macro_f1": 0.5472845960785249}, {"mcc": 0.10771122500247611, "macro_f1": 0.4494790491679266}]}, "total": {"test_mcc": 11.520993276625184, "test_mcc_se": 3.0105576801811313, "test_macro_f1": 49.37177878175003, "test_macro_f1_se": 4.115212254632394}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": -0.0009775171065493646, "macro_f1": 0.32714662790538496}, {"mcc": -0.02175978876390385, "macro_f1": 0.33657272432782637}, {"mcc": 0.001161034336867612, "macro_f1": 0.3423892371260792}, {"mcc": 0.032614924844069575, "macro_f1": 0.3387957245400119}, {"mcc": 0.026640647490867732, "macro_f1": 0.3431052058068825}, {"mcc": 0.010225884896290194, "macro_f1": 0.3391689334959197}, {"mcc": 0.006767406673349301, "macro_f1": 0.33074455029455724}, {"mcc": 0.029806278561545795, "macro_f1": 0.3393967968618745}, {"mcc": 0.0027100026044701007, "macro_f1": 0.34081099375312696}, {"mcc": -0.0202821641791121, "macro_f1": 0.34247610585803806}]}, "total": {"test_mcc": 0.66906709357895, "test_mcc_se": 1.179459329546404, "test_macro_f1": 33.80606899969701, "test_macro_f1_se": 0.3264404581051393}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.03668946531255659, "macro_f1": 0.3267680625969396}, {"mcc": 0.0, "macro_f1": 0.33571196886149857}, {"mcc": 0.031326394704867255, "macro_f1": 0.3359375}, {"mcc": 0.021443193919187108, "macro_f1": 0.327591706539075}, {"mcc": 0.0, "macro_f1": 0.32653732324893125}, {"mcc": 0.02141867674399123, "macro_f1": 0.3405513438882324}, {"mcc": 0.0, "macro_f1": 0.3361426256077796}, {"mcc": 0.0, "macro_f1": 0.32230311052283256}, {"mcc": 0.0, "macro_f1": 0.32230311052283256}, {"mcc": 0.0, "macro_f1": 0.3289646133682831}]}, "total": {"test_mcc": 1.1087773068060218, "test_mcc_se": 0.9277892406390903, "test_macro_f1": 33.02811365156405, "test_macro_f1_se": 0.39414170616753236}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.0, "macro_f1": 0.3278634722678044}, {"mcc": -0.02584423691392005, "macro_f1": 0.3315356301753012}, {"mcc": 0.015680786060850473, "macro_f1": 0.43070593208548996}, {"mcc": -0.03542504762563007, "macro_f1": 0.32857228374366754}, {"mcc": -0.023029235724798227, "macro_f1": 0.33730228902642695}, {"mcc": 0.0, "macro_f1": 0.3359273670557717}, {"mcc": -0.054453048348505804, "macro_f1": 0.3236964584069943}, {"mcc": 0.0, "macro_f1": 0.33159268929503916}, {"mcc": 0.0, "macro_f1": 0.33871488537294153}, {"mcc": -0.01551403154663551, "macro_f1": 0.3542739631620803}]}, "total": {"test_mcc": -1.3858481409863919, "test_mcc_se": 1.3041056889451377, "test_macro_f1": 34.40184970591517, "test_macro_f1_se": 1.9579699522828848}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.01648270609949152, "macro_f1": 0.46293706293706294}, {"mcc": 0.044237625380796106, "macro_f1": 0.45506762917690025}, {"mcc": -0.03163097111740883, "macro_f1": 0.45027124773960214}, {"mcc": -0.01997921972915381, "macro_f1": 0.4605919653328563}, {"mcc": 0.0, "macro_f1": 0.3397807865892972}, {"mcc": 0.0604245641870935, "macro_f1": 0.5068874412197264}, {"mcc": 0.027052036856800587, "macro_f1": 0.34873493721450854}, {"mcc": 0.04935369384616507, "macro_f1": 0.524311990926703}, {"mcc": 0.05325292049935875, "macro_f1": 0.526312526527128}, {"mcc": -0.02252217449696758, "macro_f1": 0.46953397890282356}]}, "total": {"test_mcc": 1.7667118152617531, "test_mcc_se": 2.134002465164145, "test_macro_f1": 45.44429566566609, "test_macro_f1_se": 3.9994691073078252}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.13230351312800437, "macro_f1": 0.4287308228730823}, {"mcc": 0.05456249904029655, "macro_f1": 0.4112392387507344}, {"mcc": 0.06242902745048135, "macro_f1": 0.373248127588774}, {"mcc": 0.10358015771688918, "macro_f1": 0.40805319720256095}, {"mcc": 0.12556962143309278, "macro_f1": 0.41967590542814137}, {"mcc": 0.03268042088421983, "macro_f1": 0.37684901641678137}, {"mcc": 0.20027285680841228, "macro_f1": 0.5648854652176911}, {"mcc": 0.08058786244002157, "macro_f1": 0.37167559491438723}, {"mcc": 0.08655388185951726, "macro_f1": 0.39869622629714463}, {"mcc": 0.10846936380315701, "macro_f1": 0.4669541685043915}]}, "total": {"test_mcc": 9.870092045640924, "test_mcc_se": 2.9498082676479918, "test_macro_f1": 42.200077631936885, "test_macro_f1_se": 3.5956557240191533}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.22830832848761948, "macro_f1": 0.602893131921401}, {"mcc": 0.16952675746597953, "macro_f1": 0.5333615938957186}, {"mcc": 0.1072819045586079, "macro_f1": 0.46312644085083426}, {"mcc": 0.18413954529718157, "macro_f1": 0.5917964857067926}, {"mcc": 0.18962177934945937, "macro_f1": 0.5894631084864739}, {"mcc": 0.11229751638314982, "macro_f1": 0.43301642178046673}, {"mcc": 0.25978091620807414, "macro_f1": 0.6214703274845939}, {"mcc": 0.1795774811057875, "macro_f1": 0.5891571147817214}, {"mcc": 0.25065688953043197, "macro_f1": 0.6085195639366041}, {"mcc": 0.11257697610753242, "macro_f1": 0.5548469815224958}]}, "total": {"test_mcc": 17.937680944938236, "test_mcc_se": 3.4770713783699163, "test_macro_f1": 55.87651170367103, "test_macro_f1_se": 3.9704192346412537}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.2694146319264918, "macro_f1": 0.6335342850626093}, {"mcc": 0.2617350051257287, "macro_f1": 0.6307315580042853}, {"mcc": 0.38070172696403226, "macro_f1": 0.6888009455892579}, {"mcc": 0.3571608013417317, "macro_f1": 0.6776455302793987}, {"mcc": 0.35476690993599985, "macro_f1": 0.6771622728622311}, {"mcc": 0.2774595324241904, "macro_f1": 0.6387259525124109}, {"mcc": 0.23698465795480259, "macro_f1": 0.5996569925685801}, {"mcc": 0.3425099845198039, "macro_f1": 0.6633251522431327}, {"mcc": 0.32389298230113656, "macro_f1": 0.6562234859459106}, {"mcc": 0.39938890241854585, "macro_f1": 0.696098657292687}]}, "total": {"test_mcc": 32.04015134912464, "test_mcc_se": 3.4417790206050967, "test_macro_f1": 65.61904832360503, "test_macro_f1_se": 1.8787905878629527}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"em": 50.50348567002324, "f1": 59.82029714154704}, {"em": 49.457364341085274, "f1": 58.4479733204106}, {"em": 51.931993817619784, "f1": 60.562023388416634}, {"em": 50.07788161993769, "f1": 60.144487366660755}, {"em": 48.803088803088805, "f1": 59.467244094400094}, {"em": 50.038550501156514, "f1": 60.28719460099559}, {"em": 54.290053151100985, "f1": 61.60697022307276}, {"em": 52.598913886733904, "f1": 60.8856297408514}, {"em": 52.15686274509804, "f1": 59.65325807090506}, {"em": 51.5527950310559, "f1": 60.88737056780961}]}, "total": {"test_em": 51.14109895669001, "test_em_se": 1.0313931861768566, "test_f1": 60.17624485150695, "test_f1_se": 0.5508132314011689}}, "num_model_parameters": 3212749824, "max_sequence_length": 131104, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"em": 39.1232423490488, "f1": 67.17950156232162}, {"em": 27.81954887218045, "f1": 55.49784893190784}, {"em": 33.751044277360066, "f1": 62.12244635782577}, {"em": 27.70885028949545, "f1": 58.84912160247672}, {"em": 29.498767460969596, "f1": 56.48602009489551}, {"em": 20.214521452145213, "f1": 46.073062137506845}, {"em": 20.115416323165704, "f1": 49.09414272433031}, {"em": 38.30141548709409, "f1": 65.33333192104425}, {"em": 23.893065998329156, "f1": 54.37857855090921}, {"em": 25.79564489112228, "f1": 54.1167325565849}]}, "total": {"test_em": 28.622151740091077, "test_em_se": 4.162505173443879, "test_f1": 56.91307864398029, "test_f1_se": 4.143679261148459}}, "num_model_parameters": 3212749824, "max_sequence_length": 131104, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"em": 47.09527498063517, "f1": 57.33949168280794}, {"em": 49.6124031007752, "f1": 57.805639035407175}, {"em": 48.145285935085006, "f1": 56.62609753589909}, {"em": 47.741433021806856, "f1": 56.678581035750156}, {"em": 52.12355212355212, "f1": 60.77094056527927}, {"em": 49.73014649190439, "f1": 59.122339161790684}, {"em": 49.278663629460894, "f1": 56.82494593596374}, {"em": 41.272304111714504, "f1": 55.78719874494052}, {"em": 50.11764705882353, "f1": 59.909163413934785}, {"em": 48.68012422360248, "f1": 57.441455341744685}]}, "total": {"test_em": 48.37968346773602, "test_em_se": 1.776126053871472, "test_f1": 57.83058524535181, "test_f1_se": 0.9906710561210017}}, "num_model_parameters": 3212749824, "max_sequence_length": 131104, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"em": 19.6048632218845, "f1": 45.767214625693036}, {"em": 13.615023474178404, "f1": 47.405608043721955}, {"em": 23.02936630602782, "f1": 52.838829719920774}, {"em": 21.792890262751158, "f1": 52.59980019986381}, {"em": 25.80128205128205, "f1": 52.76572990495529}, {"em": 26.033690658499236, "f1": 53.71734302619124}, {"em": 22.580645161290324, "f1": 50.93345395915304}, {"em": 26.371951219512194, "f1": 49.9749737428489}, {"em": 26.50231124807396, "f1": 49.728385046085705}, {"em": 24.430955993930198, "f1": 51.6280265478057}]}, "total": {"test_em": 22.976297959742983, "test_em_se": 2.4789440824629247, "test_f1": 50.73593648162394, "test_f1_se": 1.5884647007981765}}, "num_model_parameters": 3212749824, "max_sequence_length": 131104, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"em": 19.519752130131682, "f1": 44.63290224978412}, {"em": 26.434108527131784, "f1": 58.40932063467007}, {"em": 22.256568778979908, "f1": 45.420043643813756}, {"em": 22.97507788161994, "f1": 46.899050325803586}, {"em": 23.706563706563706, "f1": 52.28733756932006}, {"em": 23.515805705474172, "f1": 52.457312094928206}, {"em": 16.932422171602127, "f1": 40.10357265224115}, {"em": 7.292474786656323, "f1": 28.34070679449817}, {"em": 19.372549019607842, "f1": 48.397758220778115}, {"em": 19.953416149068325, "f1": 53.24727627622949}]}, "total": {"test_em": 20.19587388568358, "test_em_se": 3.2834437533976675, "test_f1": 47.01952804620667, "test_f1_se": 5.19689072754445}}, "num_model_parameters": 3212749824, "max_sequence_length": 131104, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"em": 54.918667699457785, "f1": 78.74034361270203}, {"em": 55.42635658914729, "f1": 77.98477740361675}, {"em": 49.92272024729521, "f1": 76.28104514937965}, {"em": 46.80685358255452, "f1": 73.55360447268899}, {"em": 47.87644787644788, "f1": 75.16856160778343}, {"em": 46.954510408635315, "f1": 74.18197080696586}, {"em": 54.97342444950645, "f1": 76.70776663712358}, {"em": 51.20248254460822, "f1": 74.29344257072374}, {"em": 48.470588235294116, "f1": 76.29232855439349}, {"em": 38.81987577639752, "f1": 69.42336777333094}]}, "total": {"test_em": 49.53719274093443, "test_em_se": 3.1264478873814663, "test_f1": 75.26272085887084, "test_f1_se": 1.6375640130986882}}, "num_model_parameters": 3212749824, "max_sequence_length": 131104, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"em": 47.1727343144849, "f1": 63.66918353708149}, {"em": 50.23255813953488, "f1": 66.13573113607373}, {"em": 52.31839258114374, "f1": 67.67424247373307}, {"em": 46.10591900311527, "f1": 62.12431731502419}, {"em": 47.95366795366795, "f1": 64.71872997565721}, {"em": 50.886661526599845, "f1": 66.0994534694959}, {"em": 48.747152619589976, "f1": 66.92141564227789}, {"em": 47.323506594259115, "f1": 63.3055977448994}, {"em": 43.21568627450981, "f1": 61.87871088218913}, {"em": 43.7111801242236, "f1": 61.84956404315601}]}, "total": {"test_em": 47.76674591311291, "test_em_se": 1.8246038824606758, "test_f1": 64.4376946219588, "test_f1_se": 1.3522366301050937}}, "num_model_parameters": 3212749824, "max_sequence_length": 131104, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"bertscore": 0.6667006455245428, "rouge_l": 0.20254902039373546}, {"bertscore": 0.6700816130032763, "rouge_l": 0.21113012530844547}, {"bertscore": 0.6463267653016374, "rouge_l": 0.20389814454513322}, {"bertscore": 0.6569811394438148, "rouge_l": 0.18356329175182168}, {"bertscore": 0.6288783310010331, "rouge_l": 0.18606152130802328}, {"bertscore": 0.6610827566764783, "rouge_l": 0.1928020854975715}, {"bertscore": 0.6547049144865014, "rouge_l": 0.176301386052293}, {"bertscore": 0.6486691315076314, "rouge_l": 0.16105149738595306}, {"bertscore": 0.6444895903114229, "rouge_l": 0.15885647054833252}, {"bertscore": 0.6406220135831973, "rouge_l": 0.15735569835124974}]}, "total": {"test_bertscore": 65.18536900839536, "test_bertscore_se": 0.7788711917637351, "test_rouge_l": 18.335692411425587, "test_rouge_l_se": 1.2191007697717509}}, "num_model_parameters": 3212749824, "max_sequence_length": 131328, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"bertscore": 0.6572601597436005, "rouge_l": 0.19065836731136718}, {"bertscore": 0.6486432485107798, "rouge_l": 0.18548902272389955}, {"bertscore": 0.6479701580683468, "rouge_l": 0.1816986903512896}, {"bertscore": 0.6557764685130678, "rouge_l": 0.1935672666996609}, {"bertscore": 0.6661524995870423, "rouge_l": 0.21439955928275065}, {"bertscore": 0.6579766930808546, "rouge_l": 0.19014522773441841}, {"bertscore": 0.6582229511841433, "rouge_l": 0.18793108718097118}, {"bertscore": 0.6874221376638161, "rouge_l": 0.26410159520006643}, {"bertscore": 0.6962312074610963, "rouge_l": 0.29209192712456156}, {"bertscore": 0.6639197426557075, "rouge_l": 0.2206184913943664}]}, "total": {"test_bertscore": 66.39575266468455, "test_bertscore_se": 0.9838169639635429, "test_rouge_l": 21.20701235003352, "test_rouge_l_se": 2.3266791520329533}}, "num_model_parameters": 3212749824, "max_sequence_length": 131328, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"bertscore": 0.5684296062972862, "rouge_l": 0.0641368778320143}, {"bertscore": 0.6572779958951287, "rouge_l": 0.1663572896196256}, {"bertscore": 0.6312546561821364, "rouge_l": 0.15262350837334165}, {"bertscore": 0.6276354398869444, "rouge_l": 0.15378341750478897}, {"bertscore": 0.633071401680354, "rouge_l": 0.15972675770846384}, {"bertscore": 0.6381342280656099, "rouge_l": 0.161919272090832}, {"bertscore": 0.5623300054576248, "rouge_l": 0.11947063056951325}, {"bertscore": 0.6234669584664516, "rouge_l": 0.14795681322045529}, {"bertscore": 0.6382019013690297, "rouge_l": 0.16567150777865364}, {"bertscore": 0.6201257784268819, "rouge_l": 0.14856128848824146}]}, "total": {"test_bertscore": 61.99927971727448, "test_bertscore_se": 1.8938786806326138, "test_rouge_l": 14.402073631859302, "test_rouge_l_se": 1.9297435889925714}}, "num_model_parameters": 3212749824, "max_sequence_length": 131328, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"bertscore": 0.6203160089498851, "rouge_l": 0.12003965541708109}, {"bertscore": 0.5908669547352474, "rouge_l": 0.09861127132299424}, {"bertscore": 0.6151381525342003, "rouge_l": 0.12023232597821182}, {"bertscore": 0.589563453104347, "rouge_l": 0.08953554183214238}, {"bertscore": 0.6119919738266617, "rouge_l": 0.11262790811180619}, {"bertscore": 0.6125971630972344, "rouge_l": 0.11439584448264056}, {"bertscore": 0.6080509232560871, "rouge_l": 0.10967707801952853}, {"bertscore": 0.6127445206802804, "rouge_l": 0.11594088309504014}, {"bertscore": 0.6004212239786284, "rouge_l": 0.10886817793742896}, {"bertscore": 0.5886952621585806, "rouge_l": 0.10098989613020928}]}, "total": {"test_bertscore": 60.503856363211526, "test_bertscore_se": 0.7269569652773957, "test_rouge_l": 10.909185823270832, "test_rouge_l_se": 0.6162965964017622}}, "num_model_parameters": 3212749824, "max_sequence_length": 131328, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"bertscore": 0.6734188263071701, "rouge_l": 0.19149446285861088}, {"bertscore": 0.676835489430232, "rouge_l": 0.1945246988556536}, {"bertscore": 0.6851447897352045, "rouge_l": 0.20699045443415026}, {"bertscore": 0.6892951170593733, "rouge_l": 0.2116664638513699}, {"bertscore": 0.6609905855584657, "rouge_l": 0.15247847134205664}, {"bertscore": 0.6535734770732233, "rouge_l": 0.1655810393035364}, {"bertscore": 0.6515676246053772, "rouge_l": 0.15925402562579308}, {"bertscore": 0.6742871557653416, "rouge_l": 0.18649695313407255}, {"bertscore": 0.6715891211351845, "rouge_l": 0.19707473041320417}, {"bertscore": 0.6377062897372525, "rouge_l": 0.16751739099197438}]}, "total": {"test_bertscore": 66.74408476406825, "test_bertscore_se": 1.0013550461667269, "test_rouge_l": 18.33078690810422, "test_rouge_l_se": 1.2819705109119683}}, "num_model_parameters": 3212749824, "max_sequence_length": 131328, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"bertscore": 0.6553081804013345, "rouge_l": 0.18012750488067839}, {"bertscore": 0.6522780133673223, "rouge_l": 0.17399943236103566}, {"bertscore": 0.6519677524629515, "rouge_l": 0.17820977640043595}, {"bertscore": 0.6439909693435766, "rouge_l": 0.17969059449344593}, {"bertscore": 0.6315512987202965, "rouge_l": 0.1594059246497883}, {"bertscore": 0.6519816382497083, "rouge_l": 0.17435901468283016}, {"bertscore": 0.6516590343089774, "rouge_l": 0.1752726170430428}, {"bertscore": 0.6600494420708856, "rouge_l": 0.19014964760914232}, {"bertscore": 0.6506283837225055, "rouge_l": 0.17280404317306305}, {"bertscore": 0.648635004501557, "rouge_l": 0.16854230877210644}]}, "total": {"test_bertscore": 64.98049717149115, "test_bertscore_se": 0.47259511057879566, "test_rouge_l": 17.525608640655694, "test_rouge_l_se": 0.49697893623908246}}, "num_model_parameters": 3212749824, "max_sequence_length": 131328, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"bertscore": 0.6929191083327169, "rouge_l": 0.25314053474503906}, {"bertscore": 0.690638622851111, "rouge_l": 0.24832522045613536}, {"bertscore": 0.6922211278579198, "rouge_l": 0.2427959674539935}, {"bertscore": 0.6974540348164737, "rouge_l": 0.2589379992940616}, {"bertscore": 0.691610451642191, "rouge_l": 0.2438733498386353}, {"bertscore": 0.6936923325120006, "rouge_l": 0.2431688276329187}, {"bertscore": 0.6945487826451426, "rouge_l": 0.2522663550927843}, {"bertscore": 0.6873442440992221, "rouge_l": 0.2525576446707924}, {"bertscore": 0.6941907504224218, "rouge_l": 0.25033813387009785}, {"bertscore": 0.6883417303906754, "rouge_l": 0.2381969538358613}]}, "total": {"test_bertscore": 69.22961185569875, "test_bertscore_se": 0.1859270692499287, "test_rouge_l": 24.83600986890319, "test_rouge_l_se": 0.3880964436649091}}, "num_model_parameters": 3212749824, "max_sequence_length": 131328, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.50727662667166, "accuracy": 0.6298828125}, {"mcc": 0.5163283724966924, "accuracy": 0.6376953125}, {"mcc": 0.5092657703874478, "accuracy": 0.6279296875}, {"mcc": 0.4522225947452629, "accuracy": 0.587890625}, {"mcc": 0.48470086601411067, "accuracy": 0.61328125}, {"mcc": 0.5022384297450002, "accuracy": 0.6259765625}, {"mcc": 0.4983432355972783, "accuracy": 0.623046875}, {"mcc": 0.46266886333959967, "accuracy": 0.5986328125}, {"mcc": 0.5139678458235615, "accuracy": 0.6357421875}, {"mcc": 0.5306663881750331, "accuracy": 0.6484375}]}, "total": {"test_mcc": 49.77678992995646, "test_mcc_se": 1.5195075543924272, "test_accuracy": 62.28515624999999, "test_accuracy_se": 1.1366086418130459}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.47480702838261335, "accuracy": 0.650390625}, {"mcc": 0.4276505495005222, "accuracy": 0.619140625}, {"mcc": 0.5217851299802405, "accuracy": 0.677734375}, {"mcc": 0.49377291566141374, "accuracy": 0.662109375}, {"mcc": 0.4370844254087689, "accuracy": 0.625}, {"mcc": 0.48179035841849505, "accuracy": 0.66015625}, {"mcc": 0.46780883404363943, "accuracy": 0.642578125}, {"mcc": 0.4629384557169037, "accuracy": 0.642578125}, {"mcc": 0.3824433614579479, "accuracy": 0.587890625}, {"mcc": 0.4380057459295673, "accuracy": 0.62890625}]}, "total": {"test_mcc": 45.880868045001115, "test_mcc_se": 2.4250805604625025, "test_accuracy": 63.96484375, "test_accuracy_se": 1.5919866376749494}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "mmlu-no", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.2795473567286385, "accuracy": 0.4609375}, {"mcc": 0.2898406467526693, "accuracy": 0.46923828125}, {"mcc": 0.28651724108938126, "accuracy": 0.466796875}, {"mcc": 0.2844247433092799, "accuracy": 0.466796875}, {"mcc": 0.23885584996233883, "accuracy": 0.42919921875}, {"mcc": 0.26752324327792015, "accuracy": 0.45458984375}, {"mcc": 0.26037013145863963, "accuracy": 0.44873046875}, {"mcc": 0.24909338564666056, "accuracy": 0.4384765625}, {"mcc": 0.25252656491768444, "accuracy": 0.44189453125}, {"mcc": 0.2732865166417855, "accuracy": 0.45556640625}]}, "total": {"test_mcc": 26.81985679784998, "test_mcc_se": 1.0844702820818146, "test_accuracy": 45.322265625, "test_accuracy_se": 0.8353932061076171}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.3019151645281718, "accuracy": 0.47802734375}, {"mcc": 0.29201935191649514, "accuracy": 0.47021484375}, {"mcc": 0.28885425715099033, "accuracy": 0.466796875}, {"mcc": 0.2946566598914336, "accuracy": 0.4716796875}, {"mcc": 0.3106696201943348, "accuracy": 0.484375}, {"mcc": 0.28384585600903944, "accuracy": 0.46337890625}, {"mcc": 0.3093267531234359, "accuracy": 0.4833984375}, {"mcc": 0.28922193579854033, "accuracy": 0.46826171875}, {"mcc": 0.2819270000813208, "accuracy": 0.46337890625}, {"mcc": 0.29113929057481247, "accuracy": 0.47119140625}]}, "total": {"test_mcc": 29.435758892685747, "test_mcc_se": 0.6145735123463092, "test_accuracy": 47.20703125, "test_accuracy_se": 0.4679351098266335}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.33839326401349046, "accuracy": 0.50439453125}, {"mcc": 0.33574200892725364, "accuracy": 0.50146484375}, {"mcc": 0.33478735699727324, "accuracy": 0.50244140625}, {"mcc": 0.3486289611015788, "accuracy": 0.51220703125}, {"mcc": 0.3382290602844435, "accuracy": 0.50439453125}, {"mcc": 0.34312659695412406, "accuracy": 0.5068359375}, {"mcc": 0.33139583975056885, "accuracy": 0.49951171875}, {"mcc": 0.33130117891269056, "accuracy": 0.50048828125}, {"mcc": 0.32702267018060244, "accuracy": 0.49609375}, {"mcc": 0.32928551594512334, "accuracy": 0.49951171875}]}, "total": {"test_mcc": 33.57912453067149, "test_mcc_se": 0.40854532432282875, "test_accuracy": 50.27343749999999, "test_accuracy_se": 0.2800032043273679}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.3435782180292054, "accuracy": 0.5078125}, {"mcc": 0.3280335002576642, "accuracy": 0.49609375}, {"mcc": 0.31858260302425484, "accuracy": 0.490234375}, {"mcc": 0.3486791784713766, "accuracy": 0.51123046875}, {"mcc": 0.3377879873217268, "accuracy": 0.5029296875}, {"mcc": 0.33079688419891845, "accuracy": 0.49658203125}, {"mcc": 0.33336395571648836, "accuracy": 0.498046875}, {"mcc": 0.3239732166216306, "accuracy": 0.49267578125}, {"mcc": 0.3598540765682543, "accuracy": 0.5185546875}, {"mcc": 0.3548925788371235, "accuracy": 0.515625}]}, "total": {"test_mcc": 33.79542199046643, "test_mcc_se": 0.8404010558567104, "test_accuracy": 50.29785156249999, "test_accuracy_se": 0.6112109713412395}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.4496367419204388, "accuracy": 0.5849609375}, {"mcc": 0.46651418340316836, "accuracy": 0.59765625}, {"mcc": 0.4690632159030179, "accuracy": 0.599609375}, {"mcc": 0.45072160543606726, "accuracy": 0.5859375}, {"mcc": 0.4532966166866431, "accuracy": 0.58837890625}, {"mcc": 0.47440313697742037, "accuracy": 0.60595703125}, {"mcc": 0.43536861197975574, "accuracy": 0.57470703125}, {"mcc": 0.4517875035627989, "accuracy": 0.5869140625}, {"mcc": 0.4560357583775518, "accuracy": 0.5908203125}, {"mcc": 0.4430204603265598, "accuracy": 0.58154296875}]}, "total": {"test_mcc": 45.49847834573421, "test_mcc_se": 0.7440816394514115, "test_accuracy": 58.96484374999999, "test_accuracy_se": 0.5712335598929011}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "arc-is", "task": "knowledge", "dataset_languages": ["is"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.142197102749231, "accuracy": 0.3583984375}, {"mcc": 0.15415998149772644, "accuracy": 0.369140625}, {"mcc": 0.14242106768461874, "accuracy": 0.3603515625}, {"mcc": 0.11910521277545326, "accuracy": 0.345703125}, {"mcc": 0.13704230325879801, "accuracy": 0.359375}, {"mcc": 0.10300822152483201, "accuracy": 0.330078125}, {"mcc": 0.1414370499139691, "accuracy": 0.36328125}, {"mcc": 0.1002896080159889, "accuracy": 0.328125}, {"mcc": 0.15866873765195572, "accuracy": 0.37109375}, {"mcc": 0.1350614455719346, "accuracy": 0.3564453125}]}, "total": {"test_mcc": 13.333907306445075, "test_mcc_se": 1.2280473001579804, "test_accuracy": 35.419921875, "test_accuracy_se": 0.9268007546433636}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.2011163809825723, "accuracy": 0.396484375}, {"mcc": 0.2255900432259065, "accuracy": 0.4189453125}, {"mcc": 0.21435175122197078, "accuracy": 0.40869140625}, {"mcc": 0.18716646390764308, "accuracy": 0.38427734375}, {"mcc": 0.2488120726720305, "accuracy": 0.43359375}, {"mcc": 0.237411576968545, "accuracy": 0.42626953125}, {"mcc": 0.16021578764947905, "accuracy": 0.36474609375}, {"mcc": 0.23827437046781313, "accuracy": 0.427734375}, {"mcc": 0.24658063812975856, "accuracy": 0.4326171875}, {"mcc": 0.22936634354039012, "accuracy": 0.4228515625}]}, "total": {"test_mcc": 21.88885428766109, "test_mcc_se": 1.7677577467745245, "test_accuracy": 41.162109375, "test_accuracy_se": 1.4254447793144365}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "hellaswag-no", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.18836511123348948, "accuracy": 0.384765625}, {"mcc": 0.20519381307175685, "accuracy": 0.4013671875}, {"mcc": 0.2405125925327349, "accuracy": 0.42919921875}, {"mcc": 0.2356963402587648, "accuracy": 0.42333984375}, {"mcc": 0.20280381151946067, "accuracy": 0.4013671875}, {"mcc": 0.18734366204305264, "accuracy": 0.3876953125}, {"mcc": 0.18284127968869276, "accuracy": 0.3798828125}, {"mcc": 0.24350349555762021, "accuracy": 0.43115234375}, {"mcc": 0.2015592388952731, "accuracy": 0.3896484375}, {"mcc": 0.21039052239459097, "accuracy": 0.4052734375}]}, "total": {"test_mcc": 20.98209867195436, "test_mcc_se": 1.3980022988280933, "test_accuracy": 40.3369140625, "test_accuracy_se": 1.1652791609899236}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.26855918037555715, "accuracy": 0.44970703125}, {"mcc": 0.22988189838661524, "accuracy": 0.421875}, {"mcc": 0.23963487338800096, "accuracy": 0.43017578125}, {"mcc": 0.22478078772685273, "accuracy": 0.416015625}, {"mcc": 0.1837471531012148, "accuracy": 0.38232421875}, {"mcc": 0.17216700221782677, "accuracy": 0.37158203125}, {"mcc": 0.20249191830175897, "accuracy": 0.39892578125}, {"mcc": 0.2503880337332657, "accuracy": 0.4345703125}, {"mcc": 0.24871562297227584, "accuracy": 0.42529296875}, {"mcc": 0.22181789045662617, "accuracy": 0.41455078125}]}, "total": {"test_mcc": 22.421843606599943, "test_mcc_se": 1.8874663071683486, "test_accuracy": 41.4501953125, "test_accuracy_se": 1.4871121468125852}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.03233404162249172, "accuracy": 0.5044642857142857}, {"mcc": 0.022480545378992193, "accuracy": 0.5301339285714286}, {"mcc": -0.022168866977949855, "accuracy": 0.49776785714285715}, {"mcc": 0.010382954263242152, "accuracy": 0.5189732142857143}, {"mcc": 0.0278173350028741, "accuracy": 0.5178571428571429}, {"mcc": -0.0019953072350231303, "accuracy": 0.5167410714285714}, {"mcc": -0.003495679491884857, "accuracy": 0.4966517857142857}, {"mcc": -0.01143508951419146, "accuracy": 0.49330357142857145}, {"mcc": 0.01599448556393008, "accuracy": 0.5111607142857143}, {"mcc": 0.004246264752811902, "accuracy": 0.5044642857142857}]}, "total": {"test_mcc": 0.7416068336529283, "test_mcc_se": 1.0922142148599303, "test_accuracy": 50.91517857142858, "test_accuracy_se": 0.7319322553275608}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.30927485232248636, "accuracy": 0.47802734375}, {"mcc": 0.26155876388070604, "accuracy": 0.4423828125}, {"mcc": 0.30525521514246245, "accuracy": 0.47802734375}, {"mcc": 0.28967575237817006, "accuracy": 0.46337890625}, {"mcc": 0.2641843323982274, "accuracy": 0.44287109375}, {"mcc": 0.285387515978579, "accuracy": 0.4638671875}, {"mcc": 0.2866243649980086, "accuracy": 0.462890625}, {"mcc": 0.3053769383320988, "accuracy": 0.47607421875}, {"mcc": 0.2929082629854515, "accuracy": 0.46875}, {"mcc": 0.29692363708768604, "accuracy": 0.46826171875}]}, "total": {"test_mcc": 28.971696355038766, "test_mcc_se": 1.0130697698418172, "test_accuracy": 46.4453125, "test_accuracy_se": 0.7983932784795416}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.2123658045721719, "accuracy": 0.4052734375}, {"mcc": 0.19103249768111663, "accuracy": 0.38134765625}, {"mcc": 0.2073757919511386, "accuracy": 0.39892578125}, {"mcc": 0.17600872624329497, "accuracy": 0.376953125}, {"mcc": 0.25305006945585823, "accuracy": 0.4384765625}, {"mcc": 0.20339287689881347, "accuracy": 0.3994140625}, {"mcc": 0.22759832114627288, "accuracy": 0.41845703125}, {"mcc": 0.19899926680027044, "accuracy": 0.39501953125}, {"mcc": 0.21488379836861082, "accuracy": 0.404296875}, {"mcc": 0.21701169849485386, "accuracy": 0.4091796875}]}, "total": {"test_mcc": 21.01718851612402, "test_mcc_se": 1.2948624762698997, "test_accuracy": 40.2734375, "test_accuracy_se": 1.0897649843351085}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.43004764881389856, "accuracy": 0.56640625}, {"mcc": 0.4658109471103639, "accuracy": 0.59521484375}, {"mcc": 0.4878673597357317, "accuracy": 0.60986328125}, {"mcc": 0.45222485711089155, "accuracy": 0.58544921875}, {"mcc": 0.45965480302592676, "accuracy": 0.5908203125}, {"mcc": 0.461954109019371, "accuracy": 0.595703125}, {"mcc": 0.5252385935276342, "accuracy": 0.64013671875}, {"mcc": 0.47899830424562667, "accuracy": 0.60888671875}, {"mcc": 0.46399934571737506, "accuracy": 0.59423828125}, {"mcc": 0.4244858001941005, "accuracy": 0.556640625}]}, "total": {"test_mcc": 46.5028176850092, "test_mcc_se": 1.7788585766485387, "test_accuracy": 59.43359375, "test_accuracy_se": 1.4381533051325175}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"test_speed": 2455.9, "test_speed_short": 384.09999999999997}, {"test_speed": 5252.04, "test_speed_short": 704.6999999999999}, {"test_speed": 7482.64, "test_speed_short": 1327.36}, {"test_speed": 9392.74, "test_speed_short": 1625.82}, {"test_speed": 10596.72, "test_speed_short": 1920.4999999999998}, {"test_speed": 12310.279999999999, "test_speed_short": 2393.82}, {"test_speed": 13291.3, "test_speed_short": 2664.74}, {"test_speed": 13366.44, "test_speed_short": 2949.54}, {"test_speed": 14707.140000000001, "test_speed_short": 3239.1000000000004}, {"test_speed": 15382.36, "test_speed_short": 3595.62}]}, "total": {"test_speed": 10423.756, "test_speed_se": 2640.572186637397, "test_speed_short": 2080.5299999999997, "test_speed_short_se": 666.3625683611522}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.48607447615711563, "macro_f1": 0.5720512786752403}, {"mcc": 0.44677122352585574, "macro_f1": 0.5117949240813008}, {"mcc": 0.4637436728544096, "macro_f1": 0.5757500318375831}, {"mcc": 0.4702884959871552, "macro_f1": 0.5702270184576633}, {"mcc": 0.4632321879478897, "macro_f1": 0.5666522021856718}, {"mcc": 0.4614647527520791, "macro_f1": 0.5564511037013116}, {"mcc": 0.46727071029689643, "macro_f1": 0.5464387144134899}, {"mcc": 0.47981884751804177, "macro_f1": 0.5539951903707129}, {"mcc": 0.4402752800232859, "macro_f1": 0.5193531597907536}, {"mcc": 0.4293657449302353, "macro_f1": 0.5399258420940433}]}, "total": {"test_mcc": 46.083053919929654, "test_mcc_se": 1.0841423969450124, "test_macro_f1": 55.126394656077714, "test_macro_f1_se": 1.3689448899330048}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.5630630630630631, "micro_f1": 0.35518945634266885}, {"micro_f1_no_misc": 0.5272914521112256, "micro_f1": 0.3776559287183002}, {"micro_f1_no_misc": 0.574798927613941, "micro_f1": 0.3647514525500323}, {"micro_f1_no_misc": 0.5212876427829699, "micro_f1": 0.304950495049505}, {"micro_f1_no_misc": 0.5913134484563056, "micro_f1": 0.3859987154784843}, {"micro_f1_no_misc": 0.5370871683811586, "micro_f1": 0.33191489361702126}, {"micro_f1_no_misc": 0.5230769230769231, "micro_f1": 0.32760180995475113}, {"micro_f1_no_misc": 0.5785288270377733, "micro_f1": 0.38890680425669133}, {"micro_f1_no_misc": 0.5911542610571737, "micro_f1": 0.4025378921395841}, {"micro_f1_no_misc": 0.5671641791044777, "micro_f1": 0.36665581243894496}]}, "total": {"test_micro_f1_no_misc": 55.74765892685012, "test_micro_f1_no_misc_se": 1.7245134864535188, "test_micro_f1": 36.061632605459835, "test_micro_f1_se": 1.9145925733527132}}, "num_model_parameters": 8030261248, "max_sequence_length": 8320, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.25296199196444846, "macro_f1": 0.5397851538407907}, {"mcc": 0.13683009122559472, "macro_f1": 0.397836816129371}, {"mcc": 0.21963424796200956, "macro_f1": 0.4597041853181965}, {"mcc": 0.16911539949122434, "macro_f1": 0.41925084840072185}, {"mcc": 0.2590476762057309, "macro_f1": 0.5262672078366597}, {"mcc": 0.2809911476997076, "macro_f1": 0.5289792291275277}, {"mcc": 0.21553590991178423, "macro_f1": 0.4726558322289647}, {"mcc": 0.1951363251303547, "macro_f1": 0.4655138177341299}, {"mcc": 0.20593977132027785, "macro_f1": 0.46077206999919496}, {"mcc": 0.2822185928562783, "macro_f1": 0.5637982353849512}]}, "total": {"test_mcc": 22.174111537674108, "test_mcc_se": 2.9591314541471165, "test_macro_f1": 48.34563396000508, "test_macro_f1_se": 3.363444350545693}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"em": 58.40433772269559, "f1": 63.40359614565649}, {"em": 58.29457364341085, "f1": 63.252927368522414}, {"em": 58.809891808346215, "f1": 63.875100084528185}, {"em": 58.25545171339564, "f1": 64.34806321282402}, {"em": 56.447876447876446, "f1": 62.50125635839917}, {"em": 55.82112567463377, "f1": 61.617433216598656}, {"em": 56.87167805618831, "f1": 63.56082245603879}, {"em": 59.1155934833204, "f1": 64.30880342827585}, {"em": 57.1764705882353, "f1": 62.483099906629285}, {"em": 58.307453416149066, "f1": 63.920139531438366}]}, "total": {"test_em": 57.750445255425156, "test_em_se": 0.6788267145856897, "test_f1": 63.327124170891125, "test_f1_se": 0.5482319784284982}}, "num_model_parameters": 8030261248, "max_sequence_length": 8224, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"bertscore": 0.6725231672171503, "rouge_l": 0.22214115864519107}, {"bertscore": 0.6807525188487489, "rouge_l": 0.23987588833778262}, {"bertscore": 0.6469005684921285, "rouge_l": 0.19265978398040187}, {"bertscore": 0.6722417559794849, "rouge_l": 0.22222401911917505}, {"bertscore": 0.6398101585073164, "rouge_l": 0.19918474855866947}, {"bertscore": 0.6713841050368501, "rouge_l": 0.2203194409046185}, {"bertscore": 0.655872098839609, "rouge_l": 0.18823205882569174}, {"bertscore": 0.6621481580368709, "rouge_l": 0.20064560479351934}, {"bertscore": 0.665483837670763, "rouge_l": 0.20867867151332561}, {"bertscore": 0.6691103563207434, "rouge_l": 0.21422348712289946}]}, "total": {"test_bertscore": 66.36226724949665, "test_bertscore_se": 0.7866273486113065, "test_rouge_l": 21.08184861801275, "test_rouge_l_se": 0.9881932262855728}}, "num_model_parameters": 8030261248, "max_sequence_length": 8448, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.5936370783194204, "accuracy": 0.6875}, {"mcc": 0.6180047679683889, "accuracy": 0.70703125}, {"mcc": 0.5758209643136233, "accuracy": 0.671875}, {"mcc": 0.5760329246623337, "accuracy": 0.6748046875}, {"mcc": 0.6227981772884429, "accuracy": 0.7099609375}, {"mcc": 0.6345755865987616, "accuracy": 0.71875}, {"mcc": 0.6092349285437448, "accuracy": 0.6962890625}, {"mcc": 0.5764940037351817, "accuracy": 0.6767578125}, {"mcc": 0.5829244980797267, "accuracy": 0.67578125}, {"mcc": 0.5923885874296083, "accuracy": 0.6884765625}]}, "total": {"test_mcc": 59.81911516939233, "test_mcc_se": 1.3379041285178823, "test_accuracy": 69.072265625, "test_accuracy_se": 1.0319580195888751}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.5645610326108025, "accuracy": 0.7109375}, {"mcc": 0.5395510414706138, "accuracy": 0.693359375}, {"mcc": 0.5061889395703927, "accuracy": 0.671875}, {"mcc": 0.5052771759208201, "accuracy": 0.669921875}, {"mcc": 0.537499232602917, "accuracy": 0.69140625}, {"mcc": 0.5122599780100019, "accuracy": 0.673828125}, {"mcc": 0.5780890906035653, "accuracy": 0.716796875}, {"mcc": 0.5397937416493188, "accuracy": 0.68359375}, {"mcc": 0.5523111298970127, "accuracy": 0.69921875}, {"mcc": 0.5730487911618126, "accuracy": 0.708984375}]}, "total": {"test_mcc": 54.085801534972575, "test_mcc_se": 1.6547937495395109, "test_accuracy": 69.19921875, "test_accuracy_se": 1.055418374589436}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.3556580527694889, "accuracy": 0.5029296875}, {"mcc": 0.40938896772591676, "accuracy": 0.55029296875}, {"mcc": 0.38949398090870463, "accuracy": 0.52880859375}, {"mcc": 0.3845702260929847, "accuracy": 0.529296875}, {"mcc": 0.4038746651746102, "accuracy": 0.5439453125}, {"mcc": 0.3907604067136275, "accuracy": 0.5361328125}, {"mcc": 0.44488927012577495, "accuracy": 0.578125}, {"mcc": 0.41144370372558886, "accuracy": 0.5419921875}, {"mcc": 0.43839853366616427, "accuracy": 0.56396484375}, {"mcc": 0.4156590440989671, "accuracy": 0.5625}]}, "total": {"test_mcc": 40.441368510018286, "test_mcc_se": 1.6233789038835866, "test_accuracy": 54.3798828125, "test_accuracy_se": 1.330013307649838}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"test_speed": 2128.7200000000003, "test_speed_short": 304.6}, {"test_speed": 3805.3799999999997, "test_speed_short": 527.4}, {"test_speed": 5389.34, "test_speed_short": 1003.68}, {"test_speed": 6204.94, "test_speed_short": 1151.22}, {"test_speed": 6705.36, "test_speed_short": 1372.5}, {"test_speed": 7832.5, "test_speed_short": 1739.1000000000001}, {"test_speed": 7778.08, "test_speed_short": 1925.48}, {"test_speed": 8211.18, "test_speed_short": 2132.82}, {"test_speed": 9025.0, "test_speed_short": 2345.4}, {"test_speed": 8813.98, "test_speed_short": 2560.74}]}, "total": {"test_speed": 6589.447999999999, "test_speed_se": 1396.959974435267, "test_speed_short": 1506.294, "test_speed_short_se": 472.81236162017854}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-8b", "results": {"raw": {"test": [{"mcc": 0.5184707385073467, "macro_f1": 0.6679957645105746}, {"mcc": 0.5332368057632823, "macro_f1": 0.6677638417696129}, {"mcc": 0.5136245359141334, "macro_f1": 0.6696546017286815}, {"mcc": 0.509936563354307, "macro_f1": 0.6603766983162923}, {"mcc": 0.5481813783107395, "macro_f1": 0.696000267065509}, {"mcc": 0.5619295910123131, "macro_f1": 0.6997646049622124}, {"mcc": 0.5249631251196811, "macro_f1": 0.6600191396984878}, {"mcc": 0.530552042705224, "macro_f1": 0.6631966558979637}, {"mcc": 0.46553855284916823, "macro_f1": 0.5959032466152633}, {"mcc": 0.4958317775325244, "macro_f1": 0.6465772250571682}]}, "total": {"test_mcc": 52.02265111068719, "test_mcc_se": 1.67301420422098, "test_macro_f1": 66.27252045621765, "test_macro_f1_se": 1.7675440422275706}}, "num_model_parameters": 8028033024, "max_sequence_length": 8193, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-8b", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.5799903334944416, "micro_f1": 0.30550529852675107}, {"micro_f1_no_misc": 0.4573705179282868, "micro_f1": 0.23663810689514483}, {"micro_f1_no_misc": 0.40209508460918614, "micro_f1": 0.19987934848180172}, {"micro_f1_no_misc": 0.498067840274796, "micro_f1": 0.22386895475819035}, {"micro_f1_no_misc": 0.5826001955034212, "micro_f1": 0.2754098360655738}, {"micro_f1_no_misc": 0.5693430656934306, "micro_f1": 0.2647610121836926}, {"micro_f1_no_misc": 0.4892812105926861, "micro_f1": 0.22568553955415271}, {"micro_f1_no_misc": 0.5227177990829512, "micro_f1": 0.29519610118356926}, {"micro_f1_no_misc": 0.46493837654058645, "micro_f1": 0.25406551967947205}, {"micro_f1_no_misc": 0.5701624815361891, "micro_f1": 0.2779179087365283}]}, "total": {"test_micro_f1_no_misc": 51.365669052559745, "test_micro_f1_no_misc_se": 3.829342393679035, "test_micro_f1": 25.589276260648763, "test_micro_f1_se": 2.1069781159123466}}, "num_model_parameters": 8028033024, "max_sequence_length": 8320, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-8b", "results": {"raw": {"test": [{"mcc": 0.13964896768511326, "macro_f1": 0.4050693685689698}, {"mcc": 0.1499919623383007, "macro_f1": 0.4975951710141911}, {"mcc": 0.23969023837551093, "macro_f1": 0.6197524669077719}, {"mcc": 0.1997907528291508, "macro_f1": 0.5468871237640577}, {"mcc": 0.15936994626060677, "macro_f1": 0.5481036280434582}, {"mcc": 0.19521600080464485, "macro_f1": 0.5782537067545305}, {"mcc": 0.17839154509749688, "macro_f1": 0.465764160224877}, {"mcc": 0.23674444380531495, "macro_f1": 0.5545028932794456}, {"mcc": 0.130557378033124, "macro_f1": 0.4320518359039449}, {"mcc": 0.22054084621718514, "macro_f1": 0.5698846931886267}]}, "total": {"test_mcc": 18.499420814464482, "test_mcc_se": 2.4620910957393303, "test_macro_f1": 52.17865047649873, "test_macro_f1_se": 4.278771174980301}}, "num_model_parameters": 8028033024, "max_sequence_length": 8193, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-8b", "results": {"raw": {"test": [{"em": 51.7428350116189, "f1": 61.406561851952944}, {"em": 55.968992248062015, "f1": 63.853849348793666}, {"em": 52.936630602782074, "f1": 62.283343201263925}, {"em": 50.07788161993769, "f1": 61.19203918269331}, {"em": 53.204633204633204, "f1": 63.03484777770486}, {"em": 49.65304548959136, "f1": 60.85574281796322}, {"em": 53.53075170842825, "f1": 62.11281069060598}, {"em": 51.04732350659426, "f1": 61.08039217357879}, {"em": 54.11764705882353, "f1": 62.2211544446838}, {"em": 52.09627329192546, "f1": 62.806307305530844}]}, "total": {"test_em": 52.437601374239684, "test_em_se": 1.1900664774077505, "test_f1": 62.08470487947712, "test_f1_se": 0.5983599306487652}}, "num_model_parameters": 8028033024, "max_sequence_length": 8224, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-8b", "results": {"raw": {"test": [{"bertscore": 0.669816038251156, "rouge_l": 0.2173166505142002}, {"bertscore": 0.6715888144826749, "rouge_l": 0.2245842152450252}, {"bertscore": 0.639368471558555, "rouge_l": 0.20413198783281256}, {"bertscore": 0.6699016774946358, "rouge_l": 0.22083904345670727}, {"bertscore": 0.6335730952268932, "rouge_l": 0.18587823012542173}, {"bertscore": 0.6682230423029978, "rouge_l": 0.21309382709570962}, {"bertscore": 0.6690013941406505, "rouge_l": 0.2150999992927722}, {"bertscore": 0.6684635696728947, "rouge_l": 0.21707413950146887}, {"bertscore": 0.6614573605911573, "rouge_l": 0.20112701510678455}, {"bertscore": 0.6661325375753222, "rouge_l": 0.21097765373762356}]}, "total": {"test_bertscore": 66.17526001296937, "test_bertscore_se": 0.8473285218049452, "test_rouge_l": 21.101227619085254, "test_rouge_l_se": 0.7021661106099834}}, "num_model_parameters": 8028033024, "max_sequence_length": 8448, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-8b", "results": {"raw": {"test": [{"mcc": 0.41780716959842024, "accuracy": 0.556640625}, {"mcc": 0.41852714960361687, "accuracy": 0.5546875}, {"mcc": 0.3912882072468906, "accuracy": 0.5380859375}, {"mcc": 0.41102740827108575, "accuracy": 0.5556640625}, {"mcc": 0.38863685684932764, "accuracy": 0.5341796875}, {"mcc": 0.43182528857328034, "accuracy": 0.5703125}, {"mcc": 0.4399543066514609, "accuracy": 0.572265625}, {"mcc": 0.4195925180495246, "accuracy": 0.560546875}, {"mcc": 0.3901253719943335, "accuracy": 0.5361328125}, {"mcc": 0.4264675562506547, "accuracy": 0.56640625}]}, "total": {"test_mcc": 41.35251833088594, "test_mcc_se": 1.1219023189164776, "test_accuracy": 55.44921874999999, "test_accuracy_se": 0.8691145926969096}}, "num_model_parameters": 8028033024, "max_sequence_length": 8193, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-8b", "results": {"raw": {"test": [{"mcc": 0.5284380115281809, "accuracy": 0.681640625}, {"mcc": 0.5050169057281477, "accuracy": 0.66796875}, {"mcc": 0.5355531954319913, "accuracy": 0.6875}, {"mcc": 0.5223241200295258, "accuracy": 0.6796875}, {"mcc": 0.47758233323104565, "accuracy": 0.650390625}, {"mcc": 0.5383439687521514, "accuracy": 0.693359375}, {"mcc": 0.5732457577402537, "accuracy": 0.71484375}, {"mcc": 0.5199737748554207, "accuracy": 0.6796875}, {"mcc": 0.5166038399356004, "accuracy": 0.67578125}, {"mcc": 0.5067678386092972, "accuracy": 0.669921875}]}, "total": {"test_mcc": 52.23849745841614, "test_mcc_se": 1.5520772610806033, "test_accuracy": 68.0078125, "test_accuracy_se": 1.0519413837820024}}, "num_model_parameters": 8028033024, "max_sequence_length": 8193, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-8b", "results": {"raw": {"test": [{"mcc": 0.3639091287007454, "accuracy": 0.5224609375}, {"mcc": 0.387467636298574, "accuracy": 0.54052734375}, {"mcc": 0.36445995022282335, "accuracy": 0.52392578125}, {"mcc": 0.36805710904758987, "accuracy": 0.52587890625}, {"mcc": 0.3804143497056206, "accuracy": 0.53515625}, {"mcc": 0.36030133163332995, "accuracy": 0.52001953125}, {"mcc": 0.3844231258443388, "accuracy": 0.53857421875}, {"mcc": 0.39720828910230804, "accuracy": 0.54736328125}, {"mcc": 0.3912360736173816, "accuracy": 0.54345703125}, {"mcc": 0.37049951249423135, "accuracy": 0.52880859375}]}, "total": {"test_mcc": 37.679765066669425, "test_mcc_se": 0.8045990852383786, "test_accuracy": 53.26171875, "test_accuracy_se": 0.5976317680624773}}, "num_model_parameters": 8028033024, "max_sequence_length": 8193, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "CohereForAI/aya-expanse-8b", "results": {"raw": {"test": [{"test_speed": 2168.44, "test_speed_short": 296.12}, {"test_speed": 3874.7799999999997, "test_speed_short": 522.8}, {"test_speed": 4700.16, "test_speed_short": 955.6999999999999}, {"test_speed": 6132.280000000001, "test_speed_short": 1188.6299999999999}, {"test_speed": 6373.2, "test_speed_short": 1313.2}, {"test_speed": 7111.04, "test_speed_short": 1825.5800000000002}, {"test_speed": 7735.68, "test_speed_short": 2001.1299999999999}, {"test_speed": 8346.32, "test_speed_short": 2197.88}, {"test_speed": 8314.880000000001, "test_speed_short": 2386.63}, {"test_speed": 8659.199999999999, "test_speed_short": 2589.4}]}, "total": {"test_speed": 6341.598, "test_speed_se": 1339.876715978997, "test_speed_short": 1527.707, "test_speed_short_se": 490.729158830796}}, "num_model_parameters": 8028033024, "max_sequence_length": 8193, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": {"test": [{"mcc": 0.550739651578351, "macro_f1": 0.6937341958454922}, {"mcc": 0.5592335509989161, "macro_f1": 0.6928691114239345}, {"mcc": 0.5795910100277558, "macro_f1": 0.7210187506584175}, {"mcc": 0.5544155965121577, "macro_f1": 0.6975815813434307}, {"mcc": 0.5627459350196324, "macro_f1": 0.7011979604082371}, {"mcc": 0.5779243378666301, "macro_f1": 0.7155890249764215}, {"mcc": 0.5793943965946543, "macro_f1": 0.7049626581976242}, {"mcc": 0.5464403722672116, "macro_f1": 0.6613892426579002}, {"mcc": 0.5790197695251713, "macro_f1": 0.7120874019649576}, {"mcc": 0.5497976815782402, "macro_f1": 0.682484781513993}]}, "total": {"test_mcc": 56.393023019687206, "test_mcc_se": 0.8522268784800228, "test_macro_f1": 69.82914708990407, "test_macro_f1_se": 1.0777654312545752}}, "num_model_parameters": 32296476672, "max_sequence_length": 8193, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.6502095947834188, "micro_f1": 0.43341553637484587}, {"micro_f1_no_misc": 0.5958527295810411, "micro_f1": 0.43728018757327086}, {"micro_f1_no_misc": 0.5117289313640313, "micro_f1": 0.3550869193502423}, {"micro_f1_no_misc": 0.6422018348623854, "micro_f1": 0.4118476727785613}, {"micro_f1_no_misc": 0.6257283729269385, "micro_f1": 0.4024289263041679}, {"micro_f1_no_misc": 0.6816059757236228, "micro_f1": 0.45161290322580644}, {"micro_f1_no_misc": 0.5535858178887993, "micro_f1": 0.346487962273517}, {"micro_f1_no_misc": 0.6540934419202743, "micro_f1": 0.43101482326111745}, {"micro_f1_no_misc": 0.6002716161158895, "micro_f1": 0.43205128205128207}, {"micro_f1_no_misc": 0.5997334517992003, "micro_f1": 0.39497206703910615}]}, "total": {"test_micro_f1_no_misc": 61.15011766965601, "test_micro_f1_no_misc_se": 3.1462063606859267, "test_micro_f1": 40.96198280231917, "test_micro_f1_se": 2.1948855029981384}}, "num_model_parameters": 32296476672, "max_sequence_length": 8320, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": {"test": [{"mcc": 0.3774114381456481, "macro_f1": 0.6676666472137085}, {"mcc": 0.37567817444669904, "macro_f1": 0.6796130986668736}, {"mcc": 0.4045784988666319, "macro_f1": 0.691437526898076}, {"mcc": 0.3813209226022132, "macro_f1": 0.6823769165135904}, {"mcc": 0.3186138812659614, "macro_f1": 0.6388565709146112}, {"mcc": 0.3945177358329048, "macro_f1": 0.6919741210132607}, {"mcc": 0.3637828952926766, "macro_f1": 0.6357623434739981}, {"mcc": 0.40618279753192976, "macro_f1": 0.70267131242741}, {"mcc": 0.36262090932768615, "macro_f1": 0.6511583757602134}, {"mcc": 0.3905777309119322, "macro_f1": 0.694587639627731}]}, "total": {"test_mcc": 37.75284984224283, "test_mcc_se": 1.5885028292400456, "test_macro_f1": 67.36104552509474, "test_macro_f1_se": 1.4944382323737466}}, "num_model_parameters": 32296476672, "max_sequence_length": 8193, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": {"test": [{"em": 54.6862896979086, "f1": 63.87511314990903}, {"em": 60.0, "f1": 66.44821355714862}, {"em": 58.34621329211747, "f1": 66.26433742780438}, {"em": 55.45171339563863, "f1": 65.55055830035205}, {"em": 59.45945945945946, "f1": 66.78519617571203}, {"em": 55.66692367000771, "f1": 63.93712338671046}, {"em": 59.757023538344725, "f1": 66.29748546513814}, {"em": 59.73622963537626, "f1": 66.84195943699427}, {"em": 59.13725490196079, "f1": 66.42987861811386}, {"em": 58.69565217391305, "f1": 66.2405060838564}]}, "total": {"test_em": 58.09367597647266, "test_em_se": 1.2560106089820373, "test_f1": 65.86703716017392, "test_f1_se": 0.6765437970353189}}, "num_model_parameters": 32296476672, "max_sequence_length": 8224, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": {"test": [{"bertscore": 0.6819774847681401, "rouge_l": 0.23979752068164994}, {"bertscore": 0.6834043916896917, "rouge_l": 0.24726260283829063}, {"bertscore": 0.6514953300793422, "rouge_l": 0.22500362869555263}, {"bertscore": 0.6861067005520454, "rouge_l": 0.24987755585800364}, {"bertscore": 0.6415311851014849, "rouge_l": 0.20084544271044297}, {"bertscore": 0.6833887189714005, "rouge_l": 0.24351679843823637}, {"bertscore": 0.6831773575104307, "rouge_l": 0.24307646609025685}, {"bertscore": 0.6852605931781, "rouge_l": 0.24308060586493907}, {"bertscore": 0.6808077077730559, "rouge_l": 0.23406317515993924}, {"bertscore": 0.6823581127973739, "rouge_l": 0.23832750463601665}]}, "total": {"test_bertscore": 67.59507582421065, "test_bertscore_se": 0.9770906139371104, "test_rouge_l": 23.64851300973328, "test_rouge_l_se": 0.8896245200666768}}, "num_model_parameters": 32296476672, "max_sequence_length": 8448, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": {"test": [{"mcc": 0.6588921895172084, "accuracy": 0.7412109375}, {"mcc": 0.662629834397691, "accuracy": 0.744140625}, {"mcc": 0.6586217042692106, "accuracy": 0.7421875}, {"mcc": 0.6278037406460693, "accuracy": 0.71875}, {"mcc": 0.6725641979462537, "accuracy": 0.7490234375}, {"mcc": 0.6954686375373209, "accuracy": 0.76953125}, {"mcc": 0.6376026041948465, "accuracy": 0.7265625}, {"mcc": 0.6541184897028299, "accuracy": 0.73828125}, {"mcc": 0.6700231062545792, "accuracy": 0.7490234375}, {"mcc": 0.6752394426663887, "accuracy": 0.7548828125}]}, "total": {"test_mcc": 66.12963947132397, "test_mcc_se": 1.1894718417774748, "test_accuracy": 74.3359375, "test_accuracy_se": 0.8770408058276452}}, "num_model_parameters": 32296476672, "max_sequence_length": 8193, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": {"test": [{"mcc": 0.7596927487439394, "accuracy": 0.83984375}, {"mcc": 0.6964373111955477, "accuracy": 0.798828125}, {"mcc": 0.7745205780705804, "accuracy": 0.849609375}, {"mcc": 0.7481547195356397, "accuracy": 0.83203125}, {"mcc": 0.6850227685435433, "accuracy": 0.791015625}, {"mcc": 0.7569716630135677, "accuracy": 0.83984375}, {"mcc": 0.7527150251593753, "accuracy": 0.8359375}, {"mcc": 0.7466582678481238, "accuracy": 0.83203125}, {"mcc": 0.7150726213132608, "accuracy": 0.810546875}, {"mcc": 0.7315846891760063, "accuracy": 0.822265625}]}, "total": {"test_mcc": 73.66830392599584, "test_mcc_se": 1.8061315888363954, "test_accuracy": 82.51953125, "test_accuracy_se": 1.1905543971939927}}, "num_model_parameters": 32296476672, "max_sequence_length": 8193, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": {"test": [{"mcc": 0.6931808568976738, "accuracy": 0.76904296875}, {"mcc": 0.6961541244324991, "accuracy": 0.7705078125}, {"mcc": 0.6883562358538876, "accuracy": 0.76611328125}, {"mcc": 0.7031279604287183, "accuracy": 0.7763671875}, {"mcc": 0.7205249323951286, "accuracy": 0.78857421875}, {"mcc": 0.6525453571209248, "accuracy": 0.73779296875}, {"mcc": 0.6631087110456444, "accuracy": 0.74462890625}, {"mcc": 0.6867062887608667, "accuracy": 0.76318359375}, {"mcc": 0.7015329062953098, "accuracy": 0.77294921875}, {"mcc": 0.7175428008296935, "accuracy": 0.78857421875}]}, "total": {"test_mcc": 69.22780174060347, "test_mcc_se": 1.3279501525741395, "test_accuracy": 76.77734375, "test_accuracy_se": 1.0191571525239858}}, "num_model_parameters": 32296476672, "max_sequence_length": 8193, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": {"test": [{"test_speed": 1142.64, "test_speed_short": 148.28}, {"test_speed": 1672.58, "test_speed_short": 265.59999999999997}, {"test_speed": 1901.28, "test_speed_short": 500.84}, {"test_speed": 2410.92, "test_speed_short": 615.23}, {"test_speed": 2404.6400000000003, "test_speed_short": 722.4}, {"test_speed": 2493.2, "test_speed_short": 933.88}, {"test_speed": 2812.4, "test_speed_short": 1020.0699999999999}, {"test_speed": 2714.72, "test_speed_short": 1129.76}, {"test_speed": 2622.7599999999998, "test_speed_short": 1233.21}, {"test_speed": 2706.0, "test_speed_short": 1336.5}]}, "total": {"test_speed": 2288.1139999999996, "test_speed_se": 336.6308078017931, "test_speed_short": 790.577, "test_speed_short_se": 252.16400874360798}}, "num_model_parameters": 32296476672, "max_sequence_length": 8193, "vocabulary_size": 256000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3.1-8B-Instruct-SkoleGPT-DaSlimOrca", "results": {"raw": {"test": [{"mcc": 0.4042605139096028, "macro_f1": 0.45123413905724713}, {"mcc": 0.4073461133903323, "macro_f1": 0.4530478592062736}, {"mcc": 0.3882688581405336, "macro_f1": 0.4463134465593425}, {"mcc": 0.3906864934830422, "macro_f1": 0.44551947309604273}, {"mcc": 0.3804565633960835, "macro_f1": 0.43838368304193615}, {"mcc": 0.3926070755341691, "macro_f1": 0.4447659232469359}, {"mcc": 0.39299487295639873, "macro_f1": 0.44650229215710563}, {"mcc": 0.3926476505911303, "macro_f1": 0.4466560559768129}, {"mcc": 0.4119672504320519, "macro_f1": 0.4546389373649837}, {"mcc": 0.4039656251540077, "macro_f1": 0.45072407598378184}]}, "total": {"test_mcc": 39.65201016987352, "test_mcc_se": 0.610806532847957, "test_macro_f1": 44.77785885690462, "test_macro_f1_se": 0.294309068867731}}, "num_model_parameters": 8030261248, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3.1-8B-Instruct-SkoleGPT-DaSlimOrca", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.32814197754436797, "micro_f1": 0.27224249228178504}, {"micro_f1_no_misc": 0.24715647094989243, "micro_f1": 0.22709056906224953}, {"micro_f1_no_misc": 0.19264069264069264, "micro_f1": 0.18482309124767227}, {"micro_f1_no_misc": 0.2653006201132381, "micro_f1": 0.2570642660665166}, {"micro_f1_no_misc": 0.3626647666547916, "micro_f1": 0.3349374398460058}, {"micro_f1_no_misc": 0.3628972653362897, "micro_f1": 0.327433628318584}, {"micro_f1_no_misc": 0.2823061630218688, "micro_f1": 0.2274230475705927}, {"micro_f1_no_misc": 0.2767801857585139, "micro_f1": 0.21422060164083861}, {"micro_f1_no_misc": 0.24257899776571976, "micro_f1": 0.23522595596755505}, {"micro_f1_no_misc": 0.3325396825396825, "micro_f1": 0.28980679546968685}]}, "total": {"test_micro_f1_no_misc": 28.930068223250576, "test_micro_f1_no_misc_se": 3.4731839280874492, "test_micro_f1": 25.702678874714863, "test_micro_f1_se": 3.0383034076124344}}, "num_model_parameters": 8030261248, "max_sequence_length": 131200, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3.1-8B-Instruct-SkoleGPT-DaSlimOrca", "results": {"raw": {"test": [{"mcc": 0.08375344713902443, "macro_f1": 0.37656934189623703}, {"mcc": 0.07490666127435873, "macro_f1": 0.3991168865223947}, {"mcc": 0.1251288725632889, "macro_f1": 0.4828603199621763}, {"mcc": 0.10010320608166931, "macro_f1": 0.40153609831029186}, {"mcc": 0.09842359647067078, "macro_f1": 0.4290872950045883}, {"mcc": 0.11826952056921543, "macro_f1": 0.4522235699780573}, {"mcc": 0.07323047848821716, "macro_f1": 0.3710482881525421}, {"mcc": 0.151174108525155, "macro_f1": 0.4781085530226424}, {"mcc": 0.10100744276021359, "macro_f1": 0.3998294970161978}, {"mcc": 0.13007312186251677, "macro_f1": 0.4692922696105245}]}, "total": {"test_mcc": 10.5607045573433, "test_mcc_se": 1.571775547246954, "test_macro_f1": 42.596721194756526, "test_macro_f1_se": 2.6124327421270124}}, "num_model_parameters": 8030261248, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3.1-8B-Instruct-SkoleGPT-DaSlimOrca", "results": {"raw": {"test": [{"em": 53.29202168861348, "f1": 61.01037904143104}, {"em": 54.8062015503876, "f1": 62.472972579855664}, {"em": 52.936630602782074, "f1": 60.77260112065308}, {"em": 53.271028037383175, "f1": 61.34313043308365}, {"em": 53.05019305019305, "f1": 61.049514463048226}, {"em": 51.734772552043175, "f1": 59.36478820278297}, {"em": 52.69552012148823, "f1": 61.05221754290486}, {"em": 53.76260667183863, "f1": 61.590797247527064}, {"em": 53.568627450980394, "f1": 60.634220876016485}, {"em": 53.41614906832298, "f1": 61.197839060119804}]}, "total": {"test_em": 53.253375079403284, "test_em_se": 0.4870396139496962, "test_f1": 61.048846056742285, "test_f1_se": 0.4852475755344001}}, "num_model_parameters": 8030261248, "max_sequence_length": 131104, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3.1-8B-Instruct-SkoleGPT-DaSlimOrca", "results": {"raw": {"test": [{"bertscore": 0.6710519481130177, "rouge_l": 0.22441745295235063}, {"bertscore": 0.6726573065388948, "rouge_l": 0.22955770886154359}, {"bertscore": 0.627757709895377, "rouge_l": 0.1883878760495149}, {"bertscore": 0.6555991127243033, "rouge_l": 0.209461944371444}, {"bertscore": 0.6493648399628, "rouge_l": 0.20527205777480362}, {"bertscore": 0.667895835445961, "rouge_l": 0.22073521814733577}, {"bertscore": 0.6607330682018073, "rouge_l": 0.20345244164692067}, {"bertscore": 0.6686129025183618, "rouge_l": 0.21914255280720715}, {"bertscore": 0.6531475613155635, "rouge_l": 0.19733424912973707}, {"bertscore": 0.668442035879707, "rouge_l": 0.22167672018376622}]}, "total": {"test_bertscore": 65.95262320595793, "test_bertscore_se": 0.8537033683733533, "test_rouge_l": 21.19438221924624, "test_rouge_l_se": 0.8214294452915314}}, "num_model_parameters": 8030261248, "max_sequence_length": 131328, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3.1-8B-Instruct-SkoleGPT-DaSlimOrca", "results": {"raw": {"test": [{"mcc": 0.5249109035011036, "accuracy": 0.642578125}, {"mcc": 0.5536390805637154, "accuracy": 0.6640625}, {"mcc": 0.5896033139185197, "accuracy": 0.6884765625}, {"mcc": 0.5142384429566149, "accuracy": 0.6318359375}, {"mcc": 0.519988339578778, "accuracy": 0.6396484375}, {"mcc": 0.5559706359644475, "accuracy": 0.6650390625}, {"mcc": 0.5411657902441624, "accuracy": 0.654296875}, {"mcc": 0.550446733476786, "accuracy": 0.662109375}, {"mcc": 0.5641377513489577, "accuracy": 0.673828125}, {"mcc": 0.5479191627115648, "accuracy": 0.65625}]}, "total": {"test_mcc": 54.6202015426465, "test_mcc_se": 1.3958367240786267, "test_accuracy": 65.78125, "test_accuracy_se": 1.0449526945740508}}, "num_model_parameters": 8030261248, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3.1-8B-Instruct-SkoleGPT-DaSlimOrca", "results": {"raw": {"test": [{"mcc": 0.33076906970295716, "accuracy": 0.552734375}, {"mcc": 0.3299903888728159, "accuracy": 0.552734375}, {"mcc": 0.29723530232811873, "accuracy": 0.529296875}, {"mcc": 0.34704472602201497, "accuracy": 0.56640625}, {"mcc": 0.3740852853736171, "accuracy": 0.58203125}, {"mcc": 0.25972090498142975, "accuracy": 0.51953125}, {"mcc": 0.3332938277849021, "accuracy": 0.5546875}, {"mcc": 0.35868701367318573, "accuracy": 0.57421875}, {"mcc": 0.2949348427100541, "accuracy": 0.525390625}, {"mcc": 0.3635097679851021, "accuracy": 0.58203125}]}, "total": {"test_mcc": 32.89271129434198, "test_mcc_se": 2.208040137534972, "test_accuracy": 55.39062500000001, "test_accuracy_se": 1.424602085647271}}, "num_model_parameters": 8030261248, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3.1-8B-Instruct-SkoleGPT-DaSlimOrca", "results": {"raw": {"test": [{"mcc": 0.20866456557091573, "accuracy": 0.39599609375}, {"mcc": 0.18854351880840026, "accuracy": 0.37158203125}, {"mcc": 0.19210681042644198, "accuracy": 0.388671875}, {"mcc": 0.21459632070310916, "accuracy": 0.3955078125}, {"mcc": 0.21480638682639372, "accuracy": 0.40087890625}, {"mcc": 0.22988565218214668, "accuracy": 0.4208984375}, {"mcc": 0.2288768475011086, "accuracy": 0.40771484375}, {"mcc": 0.27406638353167734, "accuracy": 0.44140625}, {"mcc": 0.22108762951487718, "accuracy": 0.38232421875}, {"mcc": 0.17787403257753853, "accuracy": 0.3720703125}]}, "total": {"test_mcc": 21.505081476426092, "test_mcc_se": 1.676247565015713, "test_accuracy": 39.7705078125, "test_accuracy_se": 1.3426700875631101}}, "num_model_parameters": 8030261248, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "ThatsGroes/Llama-3.1-8B-Instruct-SkoleGPT-DaSlimOrca", "results": {"raw": {"test": [{"test_speed": 1885.1799999999998, "test_speed_short": 306.5}, {"test_speed": 3789.1800000000003, "test_speed_short": 523.98}, {"test_speed": 5435.320000000001, "test_speed_short": 1006.4000000000001}, {"test_speed": 6227.48, "test_speed_short": 1154.58}, {"test_speed": 6661.14, "test_speed_short": 1370.0}, {"test_speed": 6993.82, "test_speed_short": 1732.5}, {"test_speed": 7839.9, "test_speed_short": 1942.5}, {"test_speed": 8204.76, "test_speed_short": 2150.04}, {"test_speed": 9089.98, "test_speed_short": 2357.1}, {"test_speed": 8741.800000000001, "test_speed_short": 2538.2}]}, "total": {"test_speed": 6486.856, "test_speed_se": 1409.6901482265052, "test_speed_short": 1508.1799999999998, "test_speed_short_se": 472.9852795231468}}, "num_model_parameters": 8030261248, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}