{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.399072095776686, "macro_f1": 0.4956553080344501}, {"mcc": 0.4152435268624868, "macro_f1": 0.5279818535078712}, {"mcc": 0.4725509003311654, "macro_f1": 0.6134480203206284}, {"mcc": 0.4450589416808148, "macro_f1": 0.586510862433019}, {"mcc": 0.4817215855063483, "macro_f1": 0.6211363548687684}, {"mcc": 0.44321075474024424, "macro_f1": 0.5515259492902045}, {"mcc": 0.444891680376046, "macro_f1": 0.5740848637429616}, {"mcc": 0.4292713028314237, "macro_f1": 0.5250211388231241}, {"mcc": 0.3221088910654179, "macro_f1": 0.3917474516983129}, {"mcc": 0.42367754777904576, "macro_f1": 0.5826489283257223}]}, "total": {"test_mcc": 42.768072269496784, "test_mcc_se": 2.7649398203380207, "test_macro_f1": 54.697607310450614, "test_macro_f1_se": 4.187553369717499}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.49785038693035255, "micro_f1": 0.38921599008366903}, {"micro_f1_no_misc": 0.4097273397199705, "micro_f1": 0.3515554213228632}, {"micro_f1_no_misc": 0.38207200587803086, "micro_f1": 0.3264340626848019}, {"micro_f1_no_misc": 0.36854866643574646, "micro_f1": 0.3087248322147651}, {"micro_f1_no_misc": 0.48003227107704716, "micro_f1": 0.38395245170876674}, {"micro_f1_no_misc": 0.4091097308488613, "micro_f1": 0.32336790726052467}, {"micro_f1_no_misc": 0.3548732595501607, "micro_f1": 0.25571875752468093}, {"micro_f1_no_misc": 0.38550826591628556, "micro_f1": 0.276997578692494}, {"micro_f1_no_misc": 0.3483825097760398, "micro_f1": 0.2873722022658193}, {"micro_f1_no_misc": 0.47626774847870174, "micro_f1": 0.346797461050202}]}, "total": {"test_micro_f1_no_misc": 41.12372184611197, "test_micro_f1_no_misc_se": 3.390597785070832, "test_micro_f1": 32.50136664808586, "test_micro_f1_se": 2.738470519394666}}, "num_model_parameters": 3212749824, "max_sequence_length": 131200, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.12125042746401757, "macro_f1": 0.47322067821889313}, {"mcc": 0.10052477241292339, "macro_f1": 0.43733740019736256}, {"mcc": 0.02480578826087154, "macro_f1": 0.3783652743875743}, {"mcc": 0.1676475925141546, "macro_f1": 0.5640943587798624}, {"mcc": 0.06504230158521394, "macro_f1": 0.4566036266129767}, {"mcc": 0.13579783296949066, "macro_f1": 0.5113258400363045}, {"mcc": 0.14294570424682812, "macro_f1": 0.5237700988704463}, {"mcc": 0.19180423003059308, "macro_f1": 0.5956969558251316}, {"mcc": 0.09456945317594954, "macro_f1": 0.5472845960785249}, {"mcc": 0.10771122500247611, "macro_f1": 0.4494790491679266}]}, "total": {"test_mcc": 11.520993276625184, "test_mcc_se": 3.0105576801811313, "test_macro_f1": 49.37177878175003, "test_macro_f1_se": 4.115212254632394}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"em": 50.50348567002324, "f1": 59.82029714154704}, {"em": 49.457364341085274, "f1": 58.4479733204106}, {"em": 51.931993817619784, "f1": 60.562023388416634}, {"em": 50.07788161993769, "f1": 60.144487366660755}, {"em": 48.803088803088805, "f1": 59.467244094400094}, {"em": 50.038550501156514, "f1": 60.28719460099559}, {"em": 54.290053151100985, "f1": 61.60697022307276}, {"em": 52.598913886733904, "f1": 60.8856297408514}, {"em": 52.15686274509804, "f1": 59.65325807090506}, {"em": 51.5527950310559, "f1": 60.88737056780961}]}, "total": {"test_em": 51.14109895669001, "test_em_se": 1.0313931861768566, "test_f1": 60.17624485150695, "test_f1_se": 0.5508132314011689}}, "num_model_parameters": 3212749824, "max_sequence_length": 131104, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"bertscore": 0.6667006455245428, "rouge_l": 0.20254902039373546}, {"bertscore": 0.6700816130032763, "rouge_l": 0.21113012530844547}, {"bertscore": 0.6463267653016374, "rouge_l": 0.20389814454513322}, {"bertscore": 0.6569811394438148, "rouge_l": 0.18356329175182168}, {"bertscore": 0.6288783310010331, "rouge_l": 0.18606152130802328}, {"bertscore": 0.6610827566764783, "rouge_l": 0.1928020854975715}, {"bertscore": 0.6547049144865014, "rouge_l": 0.176301386052293}, {"bertscore": 0.6486691315076314, "rouge_l": 0.16105149738595306}, {"bertscore": 0.6444895903114229, "rouge_l": 0.15885647054833252}, {"bertscore": 0.6406220135831973, "rouge_l": 0.15735569835124974}]}, "total": {"test_bertscore": 65.18536900839536, "test_bertscore_se": 0.7788711917637351, "test_rouge_l": 18.335692411425587, "test_rouge_l_se": 1.2191007697717509}}, "num_model_parameters": 3212749824, "max_sequence_length": 131328, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.50727662667166, "accuracy": 0.6298828125}, {"mcc": 0.5163283724966924, "accuracy": 0.6376953125}, {"mcc": 0.5092657703874478, "accuracy": 0.6279296875}, {"mcc": 0.4522225947452629, "accuracy": 0.587890625}, {"mcc": 0.48470086601411067, "accuracy": 0.61328125}, {"mcc": 0.5022384297450002, "accuracy": 0.6259765625}, {"mcc": 0.4983432355972783, "accuracy": 0.623046875}, {"mcc": 0.46266886333959967, "accuracy": 0.5986328125}, {"mcc": 0.5139678458235615, "accuracy": 0.6357421875}, {"mcc": 0.5306663881750331, "accuracy": 0.6484375}]}, "total": {"test_mcc": 49.77678992995646, "test_mcc_se": 1.5195075543924272, "test_accuracy": 62.28515624999999, "test_accuracy_se": 1.1366086418130459}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.47480702838261335, "accuracy": 0.650390625}, {"mcc": 0.4276505495005222, "accuracy": 0.619140625}, {"mcc": 0.5217851299802405, "accuracy": 0.677734375}, {"mcc": 0.49377291566141374, "accuracy": 0.662109375}, {"mcc": 0.4370844254087689, "accuracy": 0.625}, {"mcc": 0.48179035841849505, "accuracy": 0.66015625}, {"mcc": 0.46780883404363943, "accuracy": 0.642578125}, {"mcc": 0.4629384557169037, "accuracy": 0.642578125}, {"mcc": 0.3824433614579479, "accuracy": 0.587890625}, {"mcc": 0.4380057459295673, "accuracy": 0.62890625}]}, "total": {"test_mcc": 45.880868045001115, "test_mcc_se": 2.4250805604625025, "test_accuracy": 63.96484375, "test_accuracy_se": 1.5919866376749494}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "meta-llama/Llama-3.2-3B-Instruct", "results": {"raw": {"test": [{"mcc": 0.2011163809825723, "accuracy": 0.396484375}, {"mcc": 0.2255900432259065, "accuracy": 0.4189453125}, {"mcc": 0.21435175122197078, "accuracy": 0.40869140625}, {"mcc": 0.18716646390764308, "accuracy": 0.38427734375}, {"mcc": 0.2488120726720305, "accuracy": 0.43359375}, {"mcc": 0.237411576968545, "accuracy": 0.42626953125}, {"mcc": 0.16021578764947905, "accuracy": 0.36474609375}, {"mcc": 0.23827437046781313, "accuracy": 0.427734375}, {"mcc": 0.24658063812975856, "accuracy": 0.4326171875}, {"mcc": 0.22936634354039012, "accuracy": 0.4228515625}]}, "total": {"test_mcc": 21.88885428766109, "test_mcc_se": 1.7677577467745245, "test_accuracy": 41.162109375, "test_accuracy_se": 1.4254447793144365}}, "num_model_parameters": 3212749824, "max_sequence_length": 131073, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.48607447615711563, "macro_f1": 0.5720512786752403}, {"mcc": 0.44677122352585574, "macro_f1": 0.5117949240813008}, {"mcc": 0.4637436728544096, "macro_f1": 0.5757500318375831}, {"mcc": 0.4702884959871552, "macro_f1": 0.5702270184576633}, {"mcc": 0.4632321879478897, "macro_f1": 0.5666522021856718}, {"mcc": 0.4614647527520791, "macro_f1": 0.5564511037013116}, {"mcc": 0.46727071029689643, "macro_f1": 0.5464387144134899}, {"mcc": 0.47981884751804177, "macro_f1": 0.5539951903707129}, {"mcc": 0.4402752800232859, "macro_f1": 0.5193531597907536}, {"mcc": 0.4293657449302353, "macro_f1": 0.5399258420940433}]}, "total": {"test_mcc": 46.083053919929654, "test_mcc_se": 1.0841423969450124, "test_macro_f1": 55.126394656077714, "test_macro_f1_se": 1.3689448899330048}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.5630630630630631, "micro_f1": 0.35518945634266885}, {"micro_f1_no_misc": 0.5272914521112256, "micro_f1": 0.3776559287183002}, {"micro_f1_no_misc": 0.574798927613941, "micro_f1": 0.3647514525500323}, {"micro_f1_no_misc": 0.5212876427829699, "micro_f1": 0.304950495049505}, {"micro_f1_no_misc": 0.5913134484563056, "micro_f1": 0.3859987154784843}, {"micro_f1_no_misc": 0.5370871683811586, "micro_f1": 0.33191489361702126}, {"micro_f1_no_misc": 0.5230769230769231, "micro_f1": 0.32760180995475113}, {"micro_f1_no_misc": 0.5785288270377733, "micro_f1": 0.38890680425669133}, {"micro_f1_no_misc": 0.5911542610571737, "micro_f1": 0.4025378921395841}, {"micro_f1_no_misc": 0.5671641791044777, "micro_f1": 0.36665581243894496}]}, "total": {"test_micro_f1_no_misc": 55.74765892685012, "test_micro_f1_no_misc_se": 1.7245134864535188, "test_micro_f1": 36.061632605459835, "test_micro_f1_se": 1.9145925733527132}}, "num_model_parameters": 8030261248, "max_sequence_length": 8320, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.25296199196444846, "macro_f1": 0.5397851538407907}, {"mcc": 0.13683009122559472, "macro_f1": 0.397836816129371}, {"mcc": 0.21963424796200956, "macro_f1": 0.4597041853181965}, {"mcc": 0.16911539949122434, "macro_f1": 0.41925084840072185}, {"mcc": 0.2590476762057309, "macro_f1": 0.5262672078366597}, {"mcc": 0.2809911476997076, "macro_f1": 0.5289792291275277}, {"mcc": 0.21553590991178423, "macro_f1": 0.4726558322289647}, {"mcc": 0.1951363251303547, "macro_f1": 0.4655138177341299}, {"mcc": 0.20593977132027785, "macro_f1": 0.46077206999919496}, {"mcc": 0.2822185928562783, "macro_f1": 0.5637982353849512}]}, "total": {"test_mcc": 22.174111537674108, "test_mcc_se": 2.9591314541471165, "test_macro_f1": 48.34563396000508, "test_macro_f1_se": 3.363444350545693}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"em": 58.40433772269559, "f1": 63.40359614565649}, {"em": 58.29457364341085, "f1": 63.252927368522414}, {"em": 58.809891808346215, "f1": 63.875100084528185}, {"em": 58.25545171339564, "f1": 64.34806321282402}, {"em": 56.447876447876446, "f1": 62.50125635839917}, {"em": 55.82112567463377, "f1": 61.617433216598656}, {"em": 56.87167805618831, "f1": 63.56082245603879}, {"em": 59.1155934833204, "f1": 64.30880342827585}, {"em": 57.1764705882353, "f1": 62.483099906629285}, {"em": 58.307453416149066, "f1": 63.920139531438366}]}, "total": {"test_em": 57.750445255425156, "test_em_se": 0.6788267145856897, "test_f1": 63.327124170891125, "test_f1_se": 0.5482319784284982}}, "num_model_parameters": 8030261248, "max_sequence_length": 8224, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"bertscore": 0.6725231672171503, "rouge_l": 0.22214115864519107}, {"bertscore": 0.6807525188487489, "rouge_l": 0.23987588833778262}, {"bertscore": 0.6469005684921285, "rouge_l": 0.19265978398040187}, {"bertscore": 0.6722417559794849, "rouge_l": 0.22222401911917505}, {"bertscore": 0.6398101585073164, "rouge_l": 0.19918474855866947}, {"bertscore": 0.6713841050368501, "rouge_l": 0.2203194409046185}, {"bertscore": 0.655872098839609, "rouge_l": 0.18823205882569174}, {"bertscore": 0.6621481580368709, "rouge_l": 0.20064560479351934}, {"bertscore": 0.665483837670763, "rouge_l": 0.20867867151332561}, {"bertscore": 0.6691103563207434, "rouge_l": 0.21422348712289946}]}, "total": {"test_bertscore": 66.36226724949665, "test_bertscore_se": 0.7866273486113065, "test_rouge_l": 21.08184861801275, "test_rouge_l_se": 0.9881932262855728}}, "num_model_parameters": 8030261248, "max_sequence_length": 8448, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.5936370783194204, "accuracy": 0.6875}, {"mcc": 0.6180047679683889, "accuracy": 0.70703125}, {"mcc": 0.5758209643136233, "accuracy": 0.671875}, {"mcc": 0.5760329246623337, "accuracy": 0.6748046875}, {"mcc": 0.6227981772884429, "accuracy": 0.7099609375}, {"mcc": 0.6345755865987616, "accuracy": 0.71875}, {"mcc": 0.6092349285437448, "accuracy": 0.6962890625}, {"mcc": 0.5764940037351817, "accuracy": 0.6767578125}, {"mcc": 0.5829244980797267, "accuracy": 0.67578125}, {"mcc": 0.5923885874296083, "accuracy": 0.6884765625}]}, "total": {"test_mcc": 59.81911516939233, "test_mcc_se": 1.3379041285178823, "test_accuracy": 69.072265625, "test_accuracy_se": 1.0319580195888751}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.5645610326108025, "accuracy": 0.7109375}, {"mcc": 0.5395510414706138, "accuracy": 0.693359375}, {"mcc": 0.5061889395703927, "accuracy": 0.671875}, {"mcc": 0.5052771759208201, "accuracy": 0.669921875}, {"mcc": 0.537499232602917, "accuracy": 0.69140625}, {"mcc": 0.5122599780100019, "accuracy": 0.673828125}, {"mcc": 0.5780890906035653, "accuracy": 0.716796875}, {"mcc": 0.5397937416493188, "accuracy": 0.68359375}, {"mcc": 0.5523111298970127, "accuracy": 0.69921875}, {"mcc": 0.5730487911618126, "accuracy": 0.708984375}]}, "total": {"test_mcc": 54.085801534972575, "test_mcc_se": 1.6547937495395109, "test_accuracy": 69.19921875, "test_accuracy_se": 1.055418374589436}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.3556580527694889, "accuracy": 0.5029296875}, {"mcc": 0.40938896772591676, "accuracy": 0.55029296875}, {"mcc": 0.38949398090870463, "accuracy": 0.52880859375}, {"mcc": 0.3845702260929847, "accuracy": 0.529296875}, {"mcc": 0.4038746651746102, "accuracy": 0.5439453125}, {"mcc": 0.3907604067136275, "accuracy": 0.5361328125}, {"mcc": 0.44488927012577495, "accuracy": 0.578125}, {"mcc": 0.41144370372558886, "accuracy": 0.5419921875}, {"mcc": 0.43839853366616427, "accuracy": 0.56396484375}, {"mcc": 0.4156590440989671, "accuracy": 0.5625}]}, "total": {"test_mcc": 40.441368510018286, "test_mcc_se": 1.6233789038835866, "test_accuracy": 54.3798828125, "test_accuracy_se": 1.330013307649838}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"test_speed": 2128.7200000000003, "test_speed_short": 304.6}, {"test_speed": 3805.3799999999997, "test_speed_short": 527.4}, {"test_speed": 5389.34, "test_speed_short": 1003.68}, {"test_speed": 6204.94, "test_speed_short": 1151.22}, {"test_speed": 6705.36, "test_speed_short": 1372.5}, {"test_speed": 7832.5, "test_speed_short": 1739.1000000000001}, {"test_speed": 7778.08, "test_speed_short": 1925.48}, {"test_speed": 8211.18, "test_speed_short": 2132.82}, {"test_speed": 9025.0, "test_speed_short": 2345.4}, {"test_speed": 8813.98, "test_speed_short": 2560.74}]}, "total": {"test_speed": 6589.447999999999, "test_speed_se": 1396.959974435267, "test_speed_short": 1506.294, "test_speed_short_se": 472.81236162017854}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}