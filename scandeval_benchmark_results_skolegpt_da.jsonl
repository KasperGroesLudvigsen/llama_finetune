{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.48607447615711563, "macro_f1": 0.5720512786752403}, {"mcc": 0.44677122352585574, "macro_f1": 0.5117949240813008}, {"mcc": 0.4637436728544096, "macro_f1": 0.5757500318375831}, {"mcc": 0.4702884959871552, "macro_f1": 0.5702270184576633}, {"mcc": 0.4632321879478897, "macro_f1": 0.5666522021856718}, {"mcc": 0.4614647527520791, "macro_f1": 0.5564511037013116}, {"mcc": 0.46727071029689643, "macro_f1": 0.5464387144134899}, {"mcc": 0.47981884751804177, "macro_f1": 0.5539951903707129}, {"mcc": 0.4402752800232859, "macro_f1": 0.5193531597907536}, {"mcc": 0.4293657449302353, "macro_f1": 0.5399258420940433}]}, "total": {"test_mcc": 46.083053919929654, "test_mcc_se": 1.0841423969450124, "test_macro_f1": 55.126394656077714, "test_macro_f1_se": 1.3689448899330048}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.5630630630630631, "micro_f1": 0.35518945634266885}, {"micro_f1_no_misc": 0.5272914521112256, "micro_f1": 0.3776559287183002}, {"micro_f1_no_misc": 0.574798927613941, "micro_f1": 0.3647514525500323}, {"micro_f1_no_misc": 0.5212876427829699, "micro_f1": 0.304950495049505}, {"micro_f1_no_misc": 0.5913134484563056, "micro_f1": 0.3859987154784843}, {"micro_f1_no_misc": 0.5370871683811586, "micro_f1": 0.33191489361702126}, {"micro_f1_no_misc": 0.5230769230769231, "micro_f1": 0.32760180995475113}, {"micro_f1_no_misc": 0.5785288270377733, "micro_f1": 0.38890680425669133}, {"micro_f1_no_misc": 0.5911542610571737, "micro_f1": 0.4025378921395841}, {"micro_f1_no_misc": 0.5671641791044777, "micro_f1": 0.36665581243894496}]}, "total": {"test_micro_f1_no_misc": 55.74765892685012, "test_micro_f1_no_misc_se": 1.7245134864535188, "test_micro_f1": 36.061632605459835, "test_micro_f1_se": 1.9145925733527132}}, "num_model_parameters": 8030261248, "max_sequence_length": 8320, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.25296199196444846, "macro_f1": 0.5397851538407907}, {"mcc": 0.13683009122559472, "macro_f1": 0.397836816129371}, {"mcc": 0.21963424796200956, "macro_f1": 0.4597041853181965}, {"mcc": 0.16911539949122434, "macro_f1": 0.41925084840072185}, {"mcc": 0.2590476762057309, "macro_f1": 0.5262672078366597}, {"mcc": 0.2809911476997076, "macro_f1": 0.5289792291275277}, {"mcc": 0.21553590991178423, "macro_f1": 0.4726558322289647}, {"mcc": 0.1951363251303547, "macro_f1": 0.4655138177341299}, {"mcc": 0.20593977132027785, "macro_f1": 0.46077206999919496}, {"mcc": 0.2822185928562783, "macro_f1": 0.5637982353849512}]}, "total": {"test_mcc": 22.174111537674108, "test_mcc_se": 2.9591314541471165, "test_macro_f1": 48.34563396000508, "test_macro_f1_se": 3.363444350545693}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"em": 58.40433772269559, "f1": 63.40359614565649}, {"em": 58.29457364341085, "f1": 63.252927368522414}, {"em": 58.809891808346215, "f1": 63.875100084528185}, {"em": 58.25545171339564, "f1": 64.34806321282402}, {"em": 56.447876447876446, "f1": 62.50125635839917}, {"em": 55.82112567463377, "f1": 61.617433216598656}, {"em": 56.87167805618831, "f1": 63.56082245603879}, {"em": 59.1155934833204, "f1": 64.30880342827585}, {"em": 57.1764705882353, "f1": 62.483099906629285}, {"em": 58.307453416149066, "f1": 63.920139531438366}]}, "total": {"test_em": 57.750445255425156, "test_em_se": 0.6788267145856897, "test_f1": 63.327124170891125, "test_f1_se": 0.5482319784284982}}, "num_model_parameters": 8030261248, "max_sequence_length": 8224, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"bertscore": 0.6725231672171503, "rouge_l": 0.22214115864519107}, {"bertscore": 0.6807525188487489, "rouge_l": 0.23987588833778262}, {"bertscore": 0.6469005684921285, "rouge_l": 0.19265978398040187}, {"bertscore": 0.6722417559794849, "rouge_l": 0.22222401911917505}, {"bertscore": 0.6398101585073164, "rouge_l": 0.19918474855866947}, {"bertscore": 0.6713841050368501, "rouge_l": 0.2203194409046185}, {"bertscore": 0.655872098839609, "rouge_l": 0.18823205882569174}, {"bertscore": 0.6621481580368709, "rouge_l": 0.20064560479351934}, {"bertscore": 0.665483837670763, "rouge_l": 0.20867867151332561}, {"bertscore": 0.6691103563207434, "rouge_l": 0.21422348712289946}]}, "total": {"test_bertscore": 66.36226724949665, "test_bertscore_se": 0.7866273486113065, "test_rouge_l": 21.08184861801275, "test_rouge_l_se": 0.9881932262855728}}, "num_model_parameters": 8030261248, "max_sequence_length": 8448, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.5936370783194204, "accuracy": 0.6875}, {"mcc": 0.6180047679683889, "accuracy": 0.70703125}, {"mcc": 0.5758209643136233, "accuracy": 0.671875}, {"mcc": 0.5760329246623337, "accuracy": 0.6748046875}, {"mcc": 0.6227981772884429, "accuracy": 0.7099609375}, {"mcc": 0.6345755865987616, "accuracy": 0.71875}, {"mcc": 0.6092349285437448, "accuracy": 0.6962890625}, {"mcc": 0.5764940037351817, "accuracy": 0.6767578125}, {"mcc": 0.5829244980797267, "accuracy": 0.67578125}, {"mcc": 0.5923885874296083, "accuracy": 0.6884765625}]}, "total": {"test_mcc": 59.81911516939233, "test_mcc_se": 1.3379041285178823, "test_accuracy": 69.072265625, "test_accuracy_se": 1.0319580195888751}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.5645610326108025, "accuracy": 0.7109375}, {"mcc": 0.5395510414706138, "accuracy": 0.693359375}, {"mcc": 0.5061889395703927, "accuracy": 0.671875}, {"mcc": 0.5052771759208201, "accuracy": 0.669921875}, {"mcc": 0.537499232602917, "accuracy": 0.69140625}, {"mcc": 0.5122599780100019, "accuracy": 0.673828125}, {"mcc": 0.5780890906035653, "accuracy": 0.716796875}, {"mcc": 0.5397937416493188, "accuracy": 0.68359375}, {"mcc": 0.5523111298970127, "accuracy": 0.69921875}, {"mcc": 0.5730487911618126, "accuracy": 0.708984375}]}, "total": {"test_mcc": 54.085801534972575, "test_mcc_se": 1.6547937495395109, "test_accuracy": 69.19921875, "test_accuracy_se": 1.055418374589436}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"mcc": 0.3556580527694889, "accuracy": 0.5029296875}, {"mcc": 0.40938896772591676, "accuracy": 0.55029296875}, {"mcc": 0.38949398090870463, "accuracy": 0.52880859375}, {"mcc": 0.3845702260929847, "accuracy": 0.529296875}, {"mcc": 0.4038746651746102, "accuracy": 0.5439453125}, {"mcc": 0.3907604067136275, "accuracy": 0.5361328125}, {"mcc": 0.44488927012577495, "accuracy": 0.578125}, {"mcc": 0.41144370372558886, "accuracy": 0.5419921875}, {"mcc": 0.43839853366616427, "accuracy": 0.56396484375}, {"mcc": 0.4156590440989671, "accuracy": 0.5625}]}, "total": {"test_mcc": 40.441368510018286, "test_mcc_se": 1.6233789038835866, "test_accuracy": 54.3798828125, "test_accuracy_se": 1.330013307649838}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "ThatsGroes/Llama-3-8B-instruct-AI-Sweden-SkoleGPT", "results": {"raw": {"test": [{"test_speed": 2128.7200000000003, "test_speed_short": 304.6}, {"test_speed": 3805.3799999999997, "test_speed_short": 527.4}, {"test_speed": 5389.34, "test_speed_short": 1003.68}, {"test_speed": 6204.94, "test_speed_short": 1151.22}, {"test_speed": 6705.36, "test_speed_short": 1372.5}, {"test_speed": 7832.5, "test_speed_short": 1739.1000000000001}, {"test_speed": 7778.08, "test_speed_short": 1925.48}, {"test_speed": 8211.18, "test_speed_short": 2132.82}, {"test_speed": 9025.0, "test_speed_short": 2345.4}, {"test_speed": 8813.98, "test_speed_short": 2560.74}]}, "total": {"test_speed": 6589.447999999999, "test_speed_se": 1396.959974435267, "test_speed_short": 1506.294, "test_speed_short_se": 472.81236162017854}}, "num_model_parameters": 8030261248, "max_sequence_length": 8193, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "13.0.0"}